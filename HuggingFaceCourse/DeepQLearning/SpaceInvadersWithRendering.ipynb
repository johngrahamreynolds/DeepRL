{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johngrahamreynolds/DeepRL/blob/main/HuggingFaceCourse/DeepQLearning/SpaceInvadersWithRendering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7xBVPzoXxOg"
      },
      "source": [
        "# Unit 3: Deep Q-Learning with Atari Games üëæ using RL Baselines3 Zoo\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg\" alt=\"Unit 3 Thumbnail\">\n",
        "\n",
        "In this notebook, **you'll train a Deep Q-Learning agent** playing Space Invaders using [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo), a training framework based on [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "We're using the [RL-Baselines-3 Zoo integration, a vanilla version of Deep Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) with no extensions such as Double-DQN, Dueling-DQN, and Prioritized Experience Replay.\n",
        "\n",
        "‚¨áÔ∏è Here is an example of what **you will achieve** ‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9S713biXntc"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéÆ Environments:\n",
        "\n",
        "- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n",
        "\n",
        "You can see the difference between Space Invaders versions here üëâ https://gymnasium.farama.org/environments/atari/space_invaders/#variants\n",
        "\n",
        "### üìö RL-Library:\n",
        "\n",
        "- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"
      ],
      "metadata": {
        "id": "ykJiGevCMVc5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wciHGjrFYz9m"
      },
      "source": [
        "## Objectives of this notebook üèÜ\n",
        "At the end of the notebook, you will:\n",
        "- Be able to understand deeper **how RL Baselines3 Zoo works**.\n",
        "- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook is from Deep Reinforcement Learning Course\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ],
      "metadata": {
        "id": "TsnP0rjxMn1e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw6fJHIAZd-J"
      },
      "source": [
        "In this free course, you will:\n",
        "\n",
        "- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n",
        "- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n",
        "- ü§ñ Train **agents in unique environments**\n",
        "\n",
        "And more check üìö the syllabus üëâ https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n",
        "\n",
        "\n",
        "The best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vgANIBBZg1p"
      },
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "Before diving into the notebook, you need to:\n",
        "\n",
        "üî≤ üìö **[Study Deep Q-Learning by reading Unit 3](https://huggingface.co/deep-rl-course/unit3/introduction)**  ü§ó"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ],
      "metadata": {
        "id": "7kszpGFaRVhq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR0jZtYreSI5"
      },
      "source": [
        "# Let's train a Deep Q-Learning agent playing Atari' Space Invaders üëæ and upload it to the Hub.\n",
        "\n",
        "We strongly recommend students **to use Google Colab for the hands-on exercises instead of running them on their personal computers**.\n",
        "\n",
        "By using Google Colab, **you can focus on learning and experimenting without worrying about the technical aspects of setting up your environments**.\n",
        "\n",
        "To validate this hands-on for the certification process, you need to push your trained model to the Hub and **get a result of >= 200**.\n",
        "\n",
        "To find your result, go to the leaderboard and find your model, **the result = mean_reward - std of reward**\n",
        "\n",
        "For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An advice üí°\n",
        "It's better to run this colab in a copy on your Google Drive, so that **if it timeouts** you still have the saved notebook on your Google Drive and do not need to fill everything from scratch.\n",
        "\n",
        "To do that you can either do `Ctrl + S` or `File > Save a copy in Google Drive.`\n",
        "\n",
        "Also, we're going to **train it for 90 minutes with 1M timesteps**. By typing `!nvidia-smi` will tell you what GPU you're using.\n",
        "\n",
        "And if you want to train more such 10 million steps, this will take about 9 hours, potentially resulting in Colab timing out. In that case, I recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."
      ],
      "metadata": {
        "id": "Nc8BnyVEc3Ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the GPU üí™\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ],
      "metadata": {
        "id": "PU4FVzaoM6fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ],
      "metadata": {
        "id": "KV0NyFdQM9ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "jqB9T2Kv-qmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install RL-Baselines3 Zoo and its dependencies üìö\n",
        "\n",
        "If you see `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.` **this is normal and it's not a critical error** there's a conflict of version. But the packages we need are installed."
      ],
      "metadata": {
        "id": "wS_cVefO-aYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo"
      ],
      "metadata": {
        "id": "S1A_E4z3awa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig cmake ffmpeg"
      ],
      "metadata": {
        "id": "8_MllY6Om1eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S9mJiKg6SqC"
      },
      "source": [
        "To be able to use Atari games in Gymnasium we need to install atari package. And accept-rom-license to download the rom files (games files)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]"
      ],
      "metadata": {
        "id": "NsRP-lX1_2fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a virtual display üîΩ\n",
        "\n",
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install the librairies and create and run a virtual screen üñ•"
      ],
      "metadata": {
        "id": "bTpYcVZVMzUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "BE5JWP5rQIKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPgzluo9z-u"
      },
      "source": [
        "## Train our Deep Q-Learning Agent to Play Space Invaders üëæ\n",
        "\n",
        "To train an agent with RL-Baselines3-Zoo, we just need to do two things:\n",
        "\n",
        "1. Create a hyperparameter config file that will contain our training hyperparameters called `dqn.yml`.\n",
        "\n",
        "This is a template example:\n",
        "\n",
        "```\n",
        "SpaceInvadersNoFrameskip-v4:\n",
        "  env_wrapper:\n",
        "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
        "  frame_stack: 4\n",
        "  policy: 'CnnPolicy'\n",
        "  n_timesteps: !!float 1e6\n",
        "  buffer_size: 100000\n",
        "  learning_rate: !!float 1e-4\n",
        "  batch_size: 32\n",
        "  learning_starts: 100000\n",
        "  target_update_interval: 1000\n",
        "  train_freq: 4\n",
        "  gradient_steps: 1\n",
        "  exploration_fraction: 0.1\n",
        "  exploration_final_eps: 0.01\n",
        "  # If True, you need to deactivate handle_timeout_termination\n",
        "  # in the replay_buffer_kwargs\n",
        "  optimize_memory_usage: False\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VjblFSVDQOj"
      },
      "source": [
        "Here we see that:\n",
        "- We use the `Atari Wrapper` that preprocess the input (Frame reduction ,grayscale, stack 4 frames)\n",
        "- We use `CnnPolicy`, since we use Convolutional layers to process the frames\n",
        "- We train it for 10 million `n_timesteps`\n",
        "- Memory (Experience Replay) size is 100000, aka the amount of experience steps you saved to train again your agent with.\n",
        "\n",
        "üí° My advice is to **reduce the training timesteps to 1M,** which will take about 90 minutes on a P100. `!nvidia-smi` will tell you what GPU you're using. At 10 million steps, this will take about 9 hours, which could likely result in Colab timing out. I recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qTkbWrkECOJ"
      },
      "source": [
        "In terms of hyperparameters optimization, my advice is to focus on these 3 hyperparameters:\n",
        "- `learning_rate`\n",
        "- `buffer_size (Experience Memory size)`\n",
        "- `batch_size`\n",
        "\n",
        "As a good practice, you need to **check the documentation to understand what each hyperparameters does**: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn8bRTHvERRL"
      },
      "source": [
        "2. We start the training and save the models on `logs` folder üìÅ\n",
        "\n",
        "- Define the algorithm after `--algo`, where we save the model after `-f` and where the hyperparameter config is after `-c`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr1TVW4xfbz3"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.train --algo dqn --env SpaceInvadersNoFrameskip-v4  -f logs/  -c dqn.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLomIiMKQaf"
      },
      "source": [
        "## Let's evaluate our agent üëÄ\n",
        "- RL-Baselines3-Zoo provides `enjoy.py`, a python script to evaluate our agent. In most RL libraries, we call the evaluation script `enjoy.py`.\n",
        "- Let's evaluate it for 5000 timesteps üî•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co5um_KeKbBJ"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liBeTltiHJtr"
      },
      "source": [
        "## Publish our trained model on the Hub üöÄ\n",
        "Now that we saw we got good results after the training, we can publish our trained model on the hub ü§ó with one line of code.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezbHS1q3HYVV"
      },
      "source": [
        "By using `rl_zoo3.push_to_hub` **you evaluate, record a replay, generate a model card of your agent and push it to the hub**.\n",
        "\n",
        "This way:\n",
        "- You can **showcase our work** üî•\n",
        "- You can **visualize your agent playing** üëÄ\n",
        "- You can **share with the community an agent that others can use** üíæ\n",
        "- You can **access a leaderboard üèÜ to see how well your agent is performing compared to your classmates** üëâ  https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMSeZRBiHk6X"
      },
      "source": [
        "To be able to share your model with the community there are three more steps to follow:\n",
        "\n",
        "1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n",
        "\n",
        "2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
        "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O6FI0F8HnzE"
      },
      "source": [
        "- Copy the token\n",
        "- Run the cell below and past the token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppu9yePwHrZX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RVEdunPHs8B"
      },
      "source": [
        "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSLwdmvhHvjw"
      },
      "source": [
        "3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW436XnhHw1H"
      },
      "source": [
        "Let's run push_to_hub.py file to upload our trained agent to the Hub.\n",
        "\n",
        "`--repo-name `: The name of the repo\n",
        "\n",
        "`-orga`: Your Hugging Face username\n",
        "\n",
        "`-f`: Where the trained model folder is (in our case `logs`)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/select-id.png\" alt=\"Select Id\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygk2sEktTDEw"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4 -orga MarioBarbeque -f logs/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /tmp && ls"
      ],
      "metadata": {
        "id": "tXyyRZwjeSi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D4F5zsTTJ-L"
      },
      "source": [
        "###."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff89kd2HL1_s"
      },
      "source": [
        "Congrats ü•≥ you've just trained and uploaded your first Deep Q-Learning agent using RL-Baselines-3 Zoo. The script above should have displayed a link to a model repository such as https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4. When you go to this link, you can:\n",
        "\n",
        "- See a **video preview of your agent** at the right.\n",
        "- Click \"Files and versions\" to see all the files in the repository.\n",
        "- Click \"Use in stable-baselines3\" to get a code snippet that shows how to load the model.\n",
        "- A model card (`README.md` file) which gives a description of the model and the hyperparameters you used.\n",
        "\n",
        "Under the hood, the Hub uses git-based repositories (don't worry if you don't know what git is), which means you can update the model with new versions as you experiment and improve your agent.\n",
        "\n",
        "**Compare the results of your agents with your classmates** using the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) üèÜ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyRKcCYY-dIo"
      },
      "source": [
        "## Load a powerful trained model üî•\n",
        "- The Stable-Baselines3 team uploaded **more than 150 trained Deep Reinforcement Learning agents on the Hub**.\n",
        "\n",
        "You can find them here: üëâ https://huggingface.co/sb3\n",
        "\n",
        "Some examples:\n",
        "- Asteroids: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n",
        "- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n",
        "- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n",
        "- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n",
        "\n",
        "Let's load an agent playing Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-9QVFIROI5Y"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZQNY_r6NJtC"
      },
      "source": [
        "1. We download the model using `rl_zoo3.load_from_hub`, and place it in a new folder that we can call `rl_trained`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdBNZHy0NGTR"
      },
      "outputs": [],
      "source": [
        "# Download model and save it into the logs/ folder\n",
        "!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFt6hmWsNdBo"
      },
      "source": [
        "2. Let's evaluate if for 5000 timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOxs0rNuN0uS"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxMDuDfPON57"
      },
      "source": [
        "Why not trying to train your own **Deep Q-Learning Agent playing BeamRiderNoFrameskip-v4? üèÜ.**\n",
        "\n",
        "If you want to try, check https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters **in the model card, you have the hyperparameters of the trained agent.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL_ZtUgpOuY6"
      },
      "source": [
        "But finding hyperparameters can be a daunting task. Fortunately, we'll see in the next Unit, how we can **use Optuna for optimizing the Hyperparameters üî•.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqaco8W-huW"
      },
      "source": [
        "## Some additional challenges üèÜ\n",
        "The best way to learn **is to try things by your own**!\n",
        "\n",
        "In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\n",
        "\n",
        "Here's a list of environments you can try to train your agent with:\n",
        "- BeamRiderNoFrameskip-v4\n",
        "- BreakoutNoFrameskip-v4\n",
        "- EnduroNoFrameskip-v4\n",
        "- PongNoFrameskip-v4\n",
        "\n",
        "Also, **if you want to learn to implement Deep Q-Learning by yourself**, you definitely should look at CleanRL implementation: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paS-XKo4-kmu"
      },
      "source": [
        "________________________________________________________________________\n",
        "Congrats on finishing this chapter!\n",
        "\n",
        "If you‚Äôre still feel confused with all these elements...it's totally normal! **This was the same for me and for all people who studied RL.**\n",
        "\n",
        "Take time to really **grasp the material before continuing and try the additional challenges**. It‚Äôs important to master these elements and having a solid foundations.\n",
        "\n",
        "In the next unit, **we‚Äôre going to learn about [Optuna](https://optuna.org/)**. One of the most critical task in Deep Reinforcement Learning is to find a good set of training hyperparameters. And Optuna is a library that helps you to automate the search.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WRx7tO7-mvC"
      },
      "source": [
        "\n",
        "\n",
        "### This is a course built with you üë∑üèø‚Äç‚ôÄÔ∏è\n",
        "\n",
        "Finally, we want to improve and update the course iteratively with your feedback. If you have some, please fill this form üëâ https://forms.gle/3HgA7bEHwAmmLfwh9\n",
        "\n",
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See you on Bonus unit 2! üî•"
      ],
      "metadata": {
        "id": "Kc3udPT-RcXc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS3Xerx0fIMV"
      },
      "source": [
        "### Custom code to render an mp4 sample of our agent playing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model from HF\n",
        "!python -m rl_zoo3.load_from_hub --algo dqn --env SpaceInvadersNoFrameskip-v4 -orga MarioBarbeque -f logs/"
      ],
      "metadata": {
        "id": "0iMLMQZGpDZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ale-py[roms]\n",
        "!pip install gymnasium[atari]\n",
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "tKU6SDkjoV-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autorom[accept-rom-license]\n",
        "# Then import to download ROMs\n",
        "!python -c \"import ale_py.roms\""
      ],
      "metadata": {
        "id": "0_nkXDjyoXkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "print(gym.envs.registry.keys())\n",
        "# Look for Space Invaders variants"
      ],
      "metadata": {
        "id": "qnyMq9t-ofi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "metadata": {
        "id": "sQ-rSZ-SqtP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "def make_atari_env(env_id='SpaceInvadersNoFrameskip-v4', render_mode='rgb_array'):\n",
        "    \"\"\"\n",
        "    Create Atari environment with the same preprocessing as used in training\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        env = gym.make(env_id, render_mode=render_mode)\n",
        "        env = AtariWrapper(env)  # This applies the standard Atari preprocessing\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "def record_preprocessed_agent(model_path, output_path=\"enjoy.mp4\",\n",
        "                            env_id='SpaceInvadersNoFrameskip-v4', n_episodes=1):\n",
        "    \"\"\"\n",
        "    Record agent with proper Atari preprocessing pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Creating environment: {env_id}\")\n",
        "\n",
        "    # Create vectorized environment with preprocessing (for model)\n",
        "    env = DummyVecEnv([make_atari_env(env_id, render_mode='rgb_array')])\n",
        "    env = VecFrameStack(env, n_stack=4)  # Stack 4 frames as expected by model\n",
        "\n",
        "    # Create separate environment for rendering (raw frames)\n",
        "    render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    model = DQN.load(model_path, env=env)\n",
        "    print(\"Model loaded successfully\")\n",
        "\n",
        "    # Recording setup\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = 60\n",
        "    all_frames = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        # Reset both environments\n",
        "        obs = env.reset()\n",
        "        render_obs, render_info = render_env.reset()\n",
        "\n",
        "        done = False\n",
        "        episode_frames = []\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        print(f\"\\nRecording episode {episode + 1}/{n_episodes}...\")\n",
        "        print(f\"Model input shape: {obs.shape}\")\n",
        "        print(f\"Render frame shape: {render_obs.shape}\")\n",
        "\n",
        "        while not done:\n",
        "            # Get action from model using preprocessed observations\n",
        "            action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "            # Step both environments with the same action\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            render_obs, render_reward, render_terminated, render_truncated, render_info = render_env.step(action[0])\n",
        "\n",
        "            episode_reward += reward[0]\n",
        "            step_count += 1\n",
        "\n",
        "            # Capture high-resolution frame for video\n",
        "            try:\n",
        "                frame = render_env.render()\n",
        "                if frame is not None and len(frame.shape) == 3:\n",
        "                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    episode_frames.append(frame_bgr)\n",
        "            except Exception as e:\n",
        "                if step_count == 1:\n",
        "                    print(f\"Warning: Could not capture frame: {e}\")\n",
        "\n",
        "            # Safety break for very long episodes\n",
        "            if step_count > 50000:\n",
        "                print(\"Episode exceeded 50000 steps, ending...\")\n",
        "                break\n",
        "\n",
        "        all_frames.extend(episode_frames)\n",
        "        print(f\"Episode {episode + 1} completed:\")\n",
        "        print(f\"  - Steps: {step_count}\")\n",
        "        print(f\"  - Total Reward: {episode_reward:.2f}\")\n",
        "        print(f\"  - Frames captured: {len(episode_frames)}\")\n",
        "\n",
        "    # Write video\n",
        "    if all_frames:\n",
        "        h, w = all_frames[0].shape[:2]\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "        print(f\"\\nWriting {len(all_frames)} frames to {output_path}...\")\n",
        "        for i, frame in enumerate(all_frames):\n",
        "            out.write(frame)\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Progress: {i}/{len(all_frames)} frames\")\n",
        "\n",
        "        out.release()\n",
        "        print(f\"‚úì Video saved to {output_path}\")\n",
        "    else:\n",
        "        print(\"‚úó No frames captured!\")\n",
        "\n",
        "    env.close()\n",
        "    render_env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual model path\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        record_preprocessed_agent(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"enjoy.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            n_episodes=1\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Model file not found: {MODEL_PATH}\")\n",
        "        print(\"Please update MODEL_PATH with the correct path to your .zip file\")\n",
        "\n",
        "        # List files in logs directory to help find the model\n",
        "        if os.path.exists(\"logs\"):\n",
        "            print(\"\\nAvailable files in logs/:\")\n",
        "            for root, dirs, files in os.walk(\"logs\"):\n",
        "                for file in files:\n",
        "                    if file.endswith('.zip'):\n",
        "                        print(f\"  {os.path.join(root, file)}\")"
      ],
      "metadata": {
        "id": "TiipkO3iwNF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "def record_with_proper_env_matching(model_path, output_path=\"spaceinvaders_demo.mp4\",\n",
        "                                   env_id='SpaceInvadersNoFrameskip-v4', n_episodes=1):\n",
        "    \"\"\"\n",
        "    Record agent ensuring exact environment matching with training\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Loading model to inspect training environment: {model_path}\")\n",
        "\n",
        "    # First, load the model without any environment to inspect its expected spaces\n",
        "    model_data = DQN.load(model_path, device='cpu')\n",
        "    expected_obs_space = model_data.observation_space\n",
        "    expected_action_space = model_data.action_space\n",
        "\n",
        "    print(f\"Model expects observation space: {expected_obs_space}\")\n",
        "    print(f\"Model expects action space: {expected_action_space}\")\n",
        "\n",
        "    # Create environment that matches the model's expectations\n",
        "    def make_training_env():\n",
        "        env = gym.make(env_id, render_mode='rgb_array')\n",
        "        env = AtariWrapper(env)\n",
        "        return env\n",
        "\n",
        "    # Create vectorized environment with frame stacking\n",
        "    vec_env = DummyVecEnv([make_training_env])\n",
        "    vec_env = VecFrameStack(vec_env, n_stack=4)\n",
        "\n",
        "    print(f\"Created environment observation space: {vec_env.observation_space}\")\n",
        "    print(f\"Created environment action space: {vec_env.action_space}\")\n",
        "\n",
        "    # Verify spaces match\n",
        "    if vec_env.observation_space != expected_obs_space:\n",
        "        print(\"WARNING: Observation spaces don't match exactly!\")\n",
        "        print(f\"Expected: {expected_obs_space}\")\n",
        "        print(f\"Got: {vec_env.observation_space}\")\n",
        "        print(\"Attempting to load with force_reset=True...\")\n",
        "\n",
        "    # Load model with the matching environment\n",
        "    try:\n",
        "        model = DQN.load(model_path, env=vec_env, force_reset=True)\n",
        "        print(\"Model loaded successfully with force_reset=True\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load with environment: {e}\")\n",
        "        print(\"Loading model without environment and setting manually...\")\n",
        "        model = DQN.load(model_path)\n",
        "        model.set_env(vec_env)\n",
        "\n",
        "    # Create separate high-resolution environment for rendering\n",
        "    render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "    # Recording setup\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = 30\n",
        "    all_frames = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        # Reset both environments\n",
        "        obs = vec_env.reset()\n",
        "        render_obs, render_info = render_env.reset()\n",
        "\n",
        "        done = False\n",
        "        episode_frames = []\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        print(f\"\\nRecording episode {episode + 1}/{n_episodes}...\")\n",
        "        print(f\"Vectorized env obs shape: {obs.shape}\")\n",
        "        print(f\"Render env obs shape: {render_obs.shape}\")\n",
        "\n",
        "        while not done:\n",
        "            # Get action from model using preprocessed observations\n",
        "            action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "            # Step vectorized environment\n",
        "            obs, reward, done_vec, info = vec_env.step(action)\n",
        "            done = done_vec[0]\n",
        "\n",
        "            # Step render environment with the same action\n",
        "            render_action = action[0] if hasattr(action, '__len__') else action\n",
        "            render_obs, render_reward, render_term, render_trunc, render_info = render_env.step(render_action)\n",
        "\n",
        "            episode_reward += reward[0]\n",
        "            step_count += 1\n",
        "\n",
        "            # Capture high-resolution frame for video\n",
        "            try:\n",
        "                frame = render_env.render()\n",
        "                if frame is not None and len(frame.shape) == 3:\n",
        "                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    episode_frames.append(frame_bgr)\n",
        "            except Exception as e:\n",
        "                if step_count == 1:\n",
        "                    print(f\"Warning: Could not capture frame: {e}\")\n",
        "\n",
        "            # Safety break\n",
        "            if step_count > 10000:\n",
        "                print(\"Episode exceeded 10000 steps, ending...\")\n",
        "                break\n",
        "\n",
        "        all_frames.extend(episode_frames)\n",
        "        print(f\"Episode {episode + 1} completed:\")\n",
        "        print(f\"  - Steps: {step_count}\")\n",
        "        print(f\"  - Total Reward: {episode_reward:.2f}\")\n",
        "        print(f\"  - Frames captured: {len(episode_frames)}\")\n",
        "\n",
        "    # Write video\n",
        "    if all_frames:\n",
        "        h, w = all_frames[0].shape[:2]\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "        print(f\"\\nWriting {len(all_frames)} frames to {output_path}...\")\n",
        "        for i, frame in enumerate(all_frames):\n",
        "            out.write(frame)\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Progress: {i}/{len(all_frames)} frames\")\n",
        "\n",
        "        out.release()\n",
        "        print(f\"‚úì Video saved to {output_path}\")\n",
        "    else:\n",
        "        print(\"‚úó No frames captured!\")\n",
        "\n",
        "    vec_env.close()\n",
        "    render_env.close()\n",
        "\n",
        "def record_using_rl_zoo_config(model_path, output_path=\"spaceinvaders_demo.mp4\",\n",
        "                              env_id='SpaceInvadersNoFrameskip-v4', n_episodes=1):\n",
        "    \"\"\"\n",
        "    Use RL Zoo3's configuration files to recreate exact training environment\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "        from rl_zoo3.utils import get_saved_hyperparams\n",
        "        from stable_baselines3.common.utils import set_random_seed\n",
        "        import yaml\n",
        "\n",
        "        print(\"Attempting to use RL Zoo3 configuration...\")\n",
        "\n",
        "        # Set random seed\n",
        "        set_random_seed(0)\n",
        "\n",
        "        # Get model directory\n",
        "        model_dir = os.path.dirname(model_path)\n",
        "\n",
        "        # Try to load hyperparameters\n",
        "        try:\n",
        "            hyperparams, stats_path = get_saved_hyperparams(\n",
        "                model_dir,\n",
        "                norm_reward=False,\n",
        "                test_mode=True\n",
        "            )\n",
        "            print(f\"Loaded hyperparameters: {hyperparams}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load hyperparameters: {e}\")\n",
        "            hyperparams = {}\n",
        "            stats_path = None\n",
        "\n",
        "        # Load model without environment first\n",
        "        print(\"Loading model without environment...\")\n",
        "        model = ALGOS['dqn'].load(model_path)\n",
        "\n",
        "        # Create environment manually with Atari preprocessing\n",
        "        def make_env():\n",
        "            env = gym.make(env_id, render_mode='rgb_array')\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        vec_env = DummyVecEnv([make_env])\n",
        "        vec_env = VecFrameStack(vec_env, n_stack=4)\n",
        "\n",
        "        # Set the environment on the model\n",
        "        print(\"Setting environment on model...\")\n",
        "        model.set_env(vec_env)\n",
        "\n",
        "        # Create render environment\n",
        "        render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        # Continue with recording as before...\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = 30\n",
        "        all_frames = []\n",
        "\n",
        "        for episode in range(n_episodes):\n",
        "            obs = vec_env.reset()\n",
        "            render_obs, render_info = render_env.reset()\n",
        "\n",
        "            done = False\n",
        "            episode_frames = []\n",
        "            episode_reward = 0\n",
        "            step_count = 0\n",
        "\n",
        "            print(f\"\\nRecording episode {episode + 1}/{n_episodes}...\")\n",
        "\n",
        "            while not done:\n",
        "                action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "                obs, reward, done_vec, info = vec_env.step(action)\n",
        "                done = done_vec[0]\n",
        "\n",
        "                render_action = action[0] if hasattr(action, '__len__') else action\n",
        "                render_obs, render_reward, render_term, render_trunc, render_info = render_env.step(render_action)\n",
        "\n",
        "                episode_reward += reward[0]\n",
        "                step_count += 1\n",
        "\n",
        "                try:\n",
        "                    frame = render_env.render()\n",
        "                    if frame is not None and len(frame.shape) == 3:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                        episode_frames.append(frame_bgr)\n",
        "                except Exception as e:\n",
        "                    if step_count == 1:\n",
        "                        print(f\"Warning: Could not capture frame: {e}\")\n",
        "\n",
        "                if step_count > 10000:\n",
        "                    break\n",
        "\n",
        "            all_frames.extend(episode_frames)\n",
        "            print(f\"Episode {episode + 1} completed:\")\n",
        "            print(f\"  - Steps: {step_count}\")\n",
        "            print(f\"  - Total Reward: {episode_reward:.2f}\")\n",
        "            print(f\"  - Frames captured: {len(episode_frames)}\")\n",
        "\n",
        "        if all_frames:\n",
        "            h, w = all_frames[0].shape[:2]\n",
        "            out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "            print(f\"\\nWriting {len(all_frames)} frames to {output_path}...\")\n",
        "            for frame in all_frames:\n",
        "                out.write(frame)\n",
        "\n",
        "            out.release()\n",
        "            print(f\"‚úì Video saved to {output_path}\")\n",
        "\n",
        "        vec_env.close()\n",
        "        render_env.close()\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"RL Zoo3 not available: {e}\")\n",
        "        print(\"Falling back to manual environment creation...\")\n",
        "        record_with_proper_env_matching(model_path, output_path, env_id, n_episodes)\n",
        "    except Exception as e:\n",
        "        print(f\"RL Zoo3 approach failed: {e}\")\n",
        "        print(\"Falling back to manual environment creation...\")\n",
        "        record_with_proper_env_matching(model_path, output_path, env_id, n_episodes)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(\"Trying RL Zoo3 configuration approach...\")\n",
        "        record_using_rl_zoo_config(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"RL_Zoo3_enjoy.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            n_episodes=1\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Model file not found: {MODEL_PATH}\")\n",
        "\n",
        "        # Help find the correct path\n",
        "        if os.path.exists(\"logs\"):\n",
        "            print(\"\\nAvailable model files:\")\n",
        "            for root, dirs, files in os.walk(\"logs\"):\n",
        "                for file in files:\n",
        "                    if file.endswith('.zip'):\n",
        "                        print(f\"  {os.path.join(root, file)}\")"
      ],
      "metadata": {
        "id": "1qIbVRN703-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4 --no-render --n-timesteps 5000 --folder logs/"
      ],
      "metadata": {
        "id": "HHwLr9Or31Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "def compare_environments(env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Compare different environment setups to understand the discrepancy\n",
        "    \"\"\"\n",
        "    print(\"=== ENVIRONMENT COMPARISON ===\")\n",
        "\n",
        "    # 1. Raw environment (no wrappers)\n",
        "    print(\"\\n1. Raw Environment:\")\n",
        "    raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "    print(f\"   Observation space: {raw_env.observation_space}\")\n",
        "    print(f\"   Action space: {raw_env.action_space}\")\n",
        "    print(f\"   Max episode steps: {raw_env.spec.max_episode_steps}\")\n",
        "    raw_env.close()\n",
        "\n",
        "    # 2. AtariWrapper only\n",
        "    print(\"\\n2. AtariWrapper Environment:\")\n",
        "    atari_env = gym.make(env_id, render_mode='rgb_array')\n",
        "    atari_env = AtariWrapper(atari_env)\n",
        "    print(f\"   Observation space: {atari_env.observation_space}\")\n",
        "    print(f\"   Action space: {atari_env.action_space}\")\n",
        "    print(f\"   Max episode steps: {getattr(atari_env.spec, 'max_episode_steps', 'Unknown')}\")\n",
        "    atari_env.close()\n",
        "\n",
        "    # 3. Vectorized + FrameStack\n",
        "    print(\"\\n3. Vectorized + FrameStack Environment:\")\n",
        "    def make_env():\n",
        "        env = gym.make(env_id, render_mode='rgb_array')\n",
        "        env = AtariWrapper(env)\n",
        "        return env\n",
        "\n",
        "    vec_env = DummyVecEnv([make_env])\n",
        "    vec_env = VecFrameStack(vec_env, n_stack=4)\n",
        "    print(f\"   Observation space: {vec_env.observation_space}\")\n",
        "    print(f\"   Action space: {vec_env.action_space}\")\n",
        "    vec_env.close()\n",
        "\n",
        "def diagnostic_record(model_path, output_path=\"diagnostic_spaceinvaders.mp4\",\n",
        "                     env_id='SpaceInvadersNoFrameskip-v4', n_episodes=3):\n",
        "    \"\"\"\n",
        "    Record with detailed diagnostics to understand the reward/episode length discrepancy\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== DIAGNOSTIC RECORDING ===\")\n",
        "    compare_environments(env_id)\n",
        "\n",
        "    # Create environments\n",
        "    def make_model_env():\n",
        "        env = gym.make(env_id, render_mode='rgb_array')\n",
        "        env = AtariWrapper(env)\n",
        "        return env\n",
        "\n",
        "    model_env = DummyVecEnv([make_model_env])\n",
        "    model_env = VecFrameStack(model_env, n_stack=4)\n",
        "\n",
        "    # Create render environment WITHOUT AtariWrapper to see raw rewards\n",
        "    render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "    # Also create an AtariWrapper render env for comparison\n",
        "    atari_render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "    atari_render_env = AtariWrapper(atari_render_env)\n",
        "\n",
        "    print(f\"\\nModel environment obs space: {model_env.observation_space}\")\n",
        "    print(f\"Raw render environment obs space: {render_env.observation_space}\")\n",
        "    print(f\"AtariWrapper render environment obs space: {atari_render_env.observation_space}\")\n",
        "\n",
        "    # Load model\n",
        "    print(f\"\\nLoading model: {model_path}\")\n",
        "    model = DQN.load(model_path, env=model_env, force_reset=True)\n",
        "\n",
        "    # Video recording setup\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = 60\n",
        "    all_frames = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"EPISODE {episode + 1}/{n_episodes}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Reset all environments\n",
        "        model_obs = model_env.reset()\n",
        "        render_obs, render_info = render_env.reset()\n",
        "        atari_obs, atari_info = atari_render_env.reset()\n",
        "\n",
        "        done = False\n",
        "        episode_frames = []\n",
        "\n",
        "        # Tracking variables\n",
        "        model_reward = 0\n",
        "        raw_reward = 0\n",
        "        atari_reward = 0\n",
        "        step_count = 0\n",
        "        lives_info = []\n",
        "\n",
        "        while not done:\n",
        "            # Get action from model\n",
        "            action, _states = model.predict(model_obs, deterministic=True)\n",
        "\n",
        "            # Step model environment\n",
        "            model_obs, m_reward, done_vec, m_info = model_env.step(action)\n",
        "            done = done_vec[0]\n",
        "            model_reward += m_reward[0]\n",
        "\n",
        "            # Step raw render environment\n",
        "            render_action = action[0] if isinstance(action, np.ndarray) else action\n",
        "            render_obs, r_reward, r_term, r_trunc, r_info = render_env.step(render_action)\n",
        "            raw_reward += r_reward\n",
        "\n",
        "            # Step AtariWrapper render environment\n",
        "            atari_obs, a_reward, a_term, a_trunc, a_info = atari_render_env.step(render_action)\n",
        "            atari_reward += a_reward\n",
        "\n",
        "            step_count += 1\n",
        "\n",
        "            # Capture frame\n",
        "            frame = render_env.render()\n",
        "            if frame is not None:\n",
        "                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                episode_frames.append(frame_bgr)\n",
        "\n",
        "            # Log detailed info every 50 steps or when interesting things happen\n",
        "            if (step_count % 50 == 0 or\n",
        "                'lives' in r_info or\n",
        "                abs(r_reward) > 0 or\n",
        "                r_term or r_trunc or\n",
        "                done):\n",
        "\n",
        "                print(f\"Step {step_count:4d}:\")\n",
        "                print(f\"  Model: reward={m_reward[0]:6.1f}, done={done}, info={m_info}\")\n",
        "                print(f\"  Raw:   reward={r_reward:6.1f}, term={r_term}, trunc={r_trunc}, info={r_info}\")\n",
        "                print(f\"  Atari: reward={a_reward:6.1f}, term={a_term}, trunc={a_trunc}, info={a_info}\")\n",
        "\n",
        "                # Track lives if available\n",
        "                if 'lives' in r_info:\n",
        "                    lives_info.append((step_count, r_info['lives']))\n",
        "                    print(f\"  Lives: {r_info['lives']}\")\n",
        "\n",
        "            # Safety break\n",
        "            if step_count > 20000:\n",
        "                print(f\"Safety break at {step_count} steps\")\n",
        "                break\n",
        "\n",
        "        all_frames.extend(episode_frames)\n",
        "\n",
        "        print(f\"\\nEPISODE {episode + 1} SUMMARY:\")\n",
        "        print(f\"  Steps: {step_count}\")\n",
        "        print(f\"  Model Environment Reward: {model_reward:.2f}\")\n",
        "        print(f\"  Raw Environment Reward: {raw_reward:.2f}\")\n",
        "        print(f\"  AtariWrapper Environment Reward: {atari_reward:.2f}\")\n",
        "        print(f\"  Frames captured: {len(episode_frames)}\")\n",
        "        print(f\"  Lives info: {lives_info}\")\n",
        "        print(f\"  Final done reason: Model done={done}\")\n",
        "        print(f\"  Raw env final state: term={r_term}, trunc={r_trunc}\")\n",
        "        print(f\"  Atari env final state: term={a_term}, trunc={a_trunc}\")\n",
        "\n",
        "    # Write video\n",
        "    if all_frames:\n",
        "        print(f\"\\nWriting video with {len(all_frames)} frames...\")\n",
        "        h, w = all_frames[0].shape[:2]\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "        for frame in all_frames:\n",
        "            out.write(frame)\n",
        "\n",
        "        out.release()\n",
        "        print(f\"‚úÖ Diagnostic video saved to: {output_path}\")\n",
        "\n",
        "    # Cleanup\n",
        "    model_env.close()\n",
        "    render_env.close()\n",
        "    atari_render_env.close()\n",
        "\n",
        "def test_rl_zoo_environment_creation(model_path, env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Try to recreate the exact environment setup used by RL Zoo3\n",
        "    \"\"\"\n",
        "    print(\"\\n=== TESTING RL ZOO3 ENVIRONMENT SETUP ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3.utils import get_saved_hyperparams, create_test_env\n",
        "        from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "        model_dir = os.path.dirname(model_path)\n",
        "\n",
        "        # Get hyperparameters\n",
        "        try:\n",
        "            hyperparams, stats_path = get_saved_hyperparams(\n",
        "                model_dir,\n",
        "                norm_reward=False,\n",
        "                test_mode=True\n",
        "            )\n",
        "            print(f\"Loaded hyperparams: {hyperparams}\")\n",
        "            print(f\"Stats path: {stats_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load hyperparams: {e}\")\n",
        "            hyperparams = {}\n",
        "            stats_path = None\n",
        "\n",
        "        # Create test environment like RL Zoo3 does\n",
        "        env = create_test_env(\n",
        "            env_id,\n",
        "            n_envs=1,\n",
        "            stats_path=stats_path,\n",
        "            seed=0,\n",
        "            log_dir=None,\n",
        "            should_render=True,\n",
        "            hyperparams=hyperparams,\n",
        "            env_kwargs={}\n",
        "        )\n",
        "\n",
        "        print(f\"RL Zoo3 test env observation space: {env.observation_space}\")\n",
        "        print(f\"RL Zoo3 test env action space: {env.action_space}\")\n",
        "\n",
        "        # Test a few steps\n",
        "        obs, info = env.reset()\n",
        "        print(f\"Initial obs shape: {obs.shape}\")\n",
        "        print(f\"Initial info: {info}\")\n",
        "\n",
        "        for i in range(10):\n",
        "            action = env.action_space.sample()\n",
        "            obs, reward, terminated, truncated, info = env.step([action])\n",
        "            print(f\"Step {i+1}: reward={reward[0]:.1f}, term={terminated[0]}, trunc={truncated[0]}\")\n",
        "\n",
        "            if terminated[0] or truncated[0]:\n",
        "                break\n",
        "\n",
        "        env.close()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"RL Zoo3 not available for environment comparison\")\n",
        "    except Exception as e:\n",
        "        print(f\"RL Zoo3 environment test failed: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        # First test RL Zoo3 environment setup\n",
        "        test_rl_zoo_environment_creation(MODEL_PATH)\n",
        "\n",
        "        # Then run diagnostic recording\n",
        "        diagnostic_record(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"diagnostic_spaceinvaders.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            n_episodes=2\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Model not found: {MODEL_PATH}\")\n",
        "        print(\"Please update MODEL_PATH\")"
      ],
      "metadata": {
        "id": "bZf9mDCT4kYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m rl_zoo3.enjoy --algo dqn --env SpaceInvadersNoFrameskip-v4 --no-render --n-timesteps 10000 --folder logs/ --verbose 1"
      ],
      "metadata": {
        "id": "RW408BT17yf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "def create_rl_zoo_compatible_env(env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Create environment setup that matches RL Zoo3's enjoy script\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from rl_zoo3.utils import get_saved_hyperparams, create_test_env\n",
        "        from rl_zoo3 import ALGOS\n",
        "\n",
        "        # This is likely how RL Zoo3 enjoy script works\n",
        "        print(\"Creating environment using RL Zoo3 utilities...\")\n",
        "\n",
        "        # Create test environment (this will be raw, no AtariWrapper)\n",
        "        env = create_test_env(\n",
        "            env_id,\n",
        "            n_envs=1,\n",
        "            stats_path=None,\n",
        "            seed=0,\n",
        "            log_dir=None,\n",
        "            should_render=True,\n",
        "            hyperparams={'normalize': False},\n",
        "            env_kwargs={}\n",
        "        )\n",
        "\n",
        "        print(f\"RL Zoo3 env observation space: {env.observation_space}\")\n",
        "        return env, \"rl_zoo3\"\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"RL Zoo3 not available, using manual setup...\")\n",
        "        return None, \"manual\"\n",
        "\n",
        "def record_with_rl_zoo_matching(model_path, output_path=\"rl_zoo_matching.mp4\",\n",
        "                               env_id='SpaceInvadersNoFrameskip-v4', n_episodes=1):\n",
        "    \"\"\"\n",
        "    Record using the exact same setup as RL Zoo3's enjoy script\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== ATTEMPTING RL ZOO3 MATCHING SETUP ===\")\n",
        "\n",
        "    # Try to create RL Zoo3 compatible environment\n",
        "    rl_zoo_env, env_type = create_rl_zoo_compatible_env(env_id)\n",
        "\n",
        "    if rl_zoo_env is not None:\n",
        "        print(\"‚úì Using RL Zoo3 environment setup\")\n",
        "\n",
        "        # Load model with the RL Zoo3 environment\n",
        "        try:\n",
        "            # Load model without environment first to avoid conflicts\n",
        "            model = DQN.load(model_path)\n",
        "            print(\"‚úì Model loaded without environment\")\n",
        "\n",
        "            # Set the environment\n",
        "            model.set_env(rl_zoo_env)\n",
        "            print(\"‚úì Environment set on model\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with RL Zoo3 setup: {e}\")\n",
        "            print(\"Falling back to force_reset approach...\")\n",
        "            try:\n",
        "                model = DQN.load(model_path, env=rl_zoo_env, force_reset=True)\n",
        "                print(\"‚úì Model loaded with force_reset=True\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Force reset also failed: {e2}\")\n",
        "                rl_zoo_env.close()\n",
        "                return record_with_manual_setup(model_path, output_path, env_id, n_episodes)\n",
        "\n",
        "        # Create separate render environment (raw, for high-res video)\n",
        "        render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        # Run recording\n",
        "        return run_recording_session(model, rl_zoo_env, render_env, output_path, n_episodes, \"RL Zoo3\")\n",
        "\n",
        "    else:\n",
        "        print(\"Falling back to manual environment setup...\")\n",
        "        return record_with_manual_setup(model_path, output_path, env_id, n_episodes)\n",
        "\n",
        "def record_with_manual_setup(model_path, output_path, env_id, n_episodes):\n",
        "    \"\"\"\n",
        "    Fallback manual setup that tries to match RL Zoo3 behavior\n",
        "    \"\"\"\n",
        "    print(\"\\n=== USING MANUAL SETUP ===\")\n",
        "\n",
        "    from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "    from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "    # Create environment for model (preprocessed)\n",
        "    def make_env():\n",
        "        env = gym.make(env_id, render_mode='rgb_array')\n",
        "        env = AtariWrapper(env)\n",
        "        return env\n",
        "\n",
        "    model_env = DummyVecEnv([make_env])\n",
        "    model_env = VecFrameStack(model_env, n_stack=4)\n",
        "\n",
        "    # Load model\n",
        "    model = DQN.load(model_path, env=model_env, force_reset=True)\n",
        "\n",
        "    # Create render environment (raw)\n",
        "    render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "    return run_recording_session(model, model_env, render_env, output_path, n_episodes, \"Manual\")\n",
        "\n",
        "def run_recording_session(model, model_env, render_env, output_path, n_episodes, setup_type):\n",
        "    \"\"\"\n",
        "    Run the actual recording session with detailed logging\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== RECORDING WITH {setup_type} SETUP ===\")\n",
        "    print(f\"Model env obs space: {model_env.observation_space}\")\n",
        "    print(f\"Render env obs space: {render_env.observation_space}\")\n",
        "\n",
        "    # Video setup\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = 30\n",
        "    all_frames = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        print(f\"\\n--- Episode {episode + 1}/{n_episodes} ---\")\n",
        "\n",
        "        # Reset environments\n",
        "        try:\n",
        "            model_reset = model_env.reset()\n",
        "            if isinstance(model_reset, tuple):\n",
        "                model_obs, model_info = model_reset\n",
        "            else:\n",
        "                model_obs = model_reset\n",
        "                model_info = {}\n",
        "        except Exception as e:\n",
        "            print(f\"Model env reset error: {e}\")\n",
        "            continue\n",
        "\n",
        "        render_obs, render_info = render_env.reset()\n",
        "\n",
        "        print(f\"Model obs shape: {model_obs.shape if hasattr(model_obs, 'shape') else type(model_obs)}\")\n",
        "        print(f\"Render obs shape: {render_obs.shape}\")\n",
        "\n",
        "        done = False\n",
        "        episode_frames = []\n",
        "        model_reward = 0\n",
        "        render_reward = 0\n",
        "        step_count = 0\n",
        "        action_log = []\n",
        "\n",
        "        while not done and step_count < 15000:\n",
        "            # Get action from model\n",
        "            action, _states = model.predict(model_obs, deterministic=True)\n",
        "            action_log.append(action)\n",
        "\n",
        "            # Step model environment\n",
        "            try:\n",
        "                model_result = model_env.step(action)\n",
        "\n",
        "                if len(model_result) == 5:\n",
        "                    model_obs, m_reward, m_terminated, m_truncated, m_info = model_result\n",
        "                    m_done = m_terminated[0] or m_truncated[0] if hasattr(m_terminated, '__len__') else m_terminated or m_truncated\n",
        "                elif len(model_result) == 4:\n",
        "                    model_obs, m_reward, m_done, m_info = model_result\n",
        "                    if hasattr(m_done, '__len__'):\n",
        "                        m_done = m_done[0]\n",
        "\n",
        "                if hasattr(m_reward, '__len__'):\n",
        "                    m_reward = m_reward[0]\n",
        "\n",
        "                model_reward += m_reward\n",
        "                done = m_done\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Model step error at step {step_count}: {e}\")\n",
        "                break\n",
        "\n",
        "            # Step render environment with same action\n",
        "            render_action = action[0] if hasattr(action, '__len__') else action\n",
        "            render_obs, r_reward, r_term, r_trunc, r_info = render_env.step(render_action)\n",
        "            render_reward += r_reward\n",
        "\n",
        "            step_count += 1\n",
        "\n",
        "            # Capture frame\n",
        "            try:\n",
        "                frame = render_env.render()\n",
        "                if frame is not None:\n",
        "                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    episode_frames.append(frame_bgr)\n",
        "            except Exception as e:\n",
        "                if step_count <= 3:\n",
        "                    print(f\"Frame capture error: {e}\")\n",
        "\n",
        "            # Log interesting events\n",
        "            if (step_count <= 10 or\n",
        "                step_count % 500 == 0 or\n",
        "                abs(r_reward) > 0 or\n",
        "                abs(m_reward) > 0 or\n",
        "                done):\n",
        "\n",
        "                print(f\"Step {step_count:4d}: model_r={m_reward:5.1f}, render_r={r_reward:5.1f}, done={done}, action={render_action}\")\n",
        "\n",
        "                if 'lives' in r_info:\n",
        "                    print(f\"           lives={r_info['lives']}\")\n",
        "\n",
        "        all_frames.extend(episode_frames)\n",
        "\n",
        "        print(f\"\\nEpisode {episode + 1} Results:\")\n",
        "        print(f\"  Duration: {step_count} steps\")\n",
        "        print(f\"  Model Total Reward: {model_reward:.2f}\")\n",
        "        print(f\"  Render Total Reward: {render_reward:.2f}\")\n",
        "        print(f\"  Frames: {len(episode_frames)}\")\n",
        "        print(f\"  Ended: {'Done flag' if done else 'Safety limit'}\")\n",
        "        print(f\"  Actions used: {len(set(action_log))} unique actions out of {len(action_log)}\")\n",
        "\n",
        "    # Write video\n",
        "    if all_frames:\n",
        "        print(f\"\\nSaving video with {len(all_frames)} frames...\")\n",
        "        h, w = all_frames[0].shape[:2]\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "        for frame in all_frames:\n",
        "            out.write(frame)\n",
        "\n",
        "        out.release()\n",
        "        print(f\"‚úÖ Video saved: {output_path}\")\n",
        "\n",
        "        # File info\n",
        "        file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "        print(f\"üìÅ Size: {file_size:.1f} MB, Length: {len(all_frames)/fps:.1f}s\")\n",
        "\n",
        "    # Cleanup\n",
        "    model_env.close()\n",
        "    render_env.close()\n",
        "\n",
        "    return True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        success = record_with_rl_zoo_matching(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"rl_zoo_matching_spaceinvaders.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            n_episodes=1\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            print(\"\\nüéØ Recording completed! Compare this with RL Zoo3 enjoy results.\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Recording failed. Check the error messages above.\")\n",
        "    else:\n",
        "        print(f\"Model not found: {MODEL_PATH}\")"
      ],
      "metadata": {
        "id": "fZ8wVV4C9U5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "def record_exactly_like_rl_zoo3(model_path, output_path=\"true_rl_zoo3_match.mp4\",\n",
        "                               env_id='SpaceInvadersNoFrameskip-v4', n_timesteps=5000):\n",
        "    \"\"\"\n",
        "    Record agent using EXACTLY the same setup as RL Zoo3's enjoy script\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== EXACT RL ZOO3 REPLICATION ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS, create_test_env, get_saved_hyperparams\n",
        "        from rl_zoo3.exp_manager import ExperimentManager\n",
        "        from rl_zoo3.utils import get_model_path\n",
        "        import yaml\n",
        "\n",
        "        # Replicate RL Zoo3's setup exactly\n",
        "        algo = 'dqn'\n",
        "        seed = 0\n",
        "        set_random_seed(seed)\n",
        "\n",
        "        # Get model path info (like RL Zoo3 does)\n",
        "        model_dir = os.path.dirname(model_path)\n",
        "        log_path = os.path.dirname(model_dir)\n",
        "\n",
        "        print(f\"Model path: {model_path}\")\n",
        "        print(f\"Log path: {log_path}\")\n",
        "\n",
        "        # Check if this is Atari (like RL Zoo3 does)\n",
        "        is_atari = ExperimentManager.is_atari(env_id)\n",
        "        print(f\"Is Atari: {is_atari}\")\n",
        "\n",
        "        # Get hyperparams exactly like RL Zoo3\n",
        "        stats_path = os.path.join(log_path, env_id)\n",
        "        hyperparams, maybe_stats_path = get_saved_hyperparams(\n",
        "            stats_path,\n",
        "            norm_reward=False,\n",
        "            test_mode=True\n",
        "        )\n",
        "\n",
        "        print(f\"Hyperparams: {hyperparams}\")\n",
        "        print(f\"Stats path: {maybe_stats_path}\")\n",
        "\n",
        "        # Load env_kwargs exactly like RL Zoo3\n",
        "        env_kwargs = {}\n",
        "        args_path = os.path.join(log_path, env_id, \"args.yml\")\n",
        "        if os.path.isfile(args_path):\n",
        "            with open(args_path) as f:\n",
        "                loaded_args = yaml.load(f, Loader=yaml.UnsafeLoader)\n",
        "                if loaded_args.get(\"env_kwargs\") is not None:\n",
        "                    env_kwargs = loaded_args[\"env_kwargs\"]\n",
        "\n",
        "        print(f\"Env kwargs: {env_kwargs}\")\n",
        "\n",
        "        # Create environment EXACTLY like RL Zoo3\n",
        "        env = create_test_env(\n",
        "            env_id,\n",
        "            n_envs=1,\n",
        "            stats_path=maybe_stats_path,\n",
        "            seed=seed,\n",
        "            log_dir=None,\n",
        "            should_render=True,  # Enable rendering\n",
        "            hyperparams=hyperparams,\n",
        "            env_kwargs=env_kwargs,\n",
        "            vec_env_cls=ExperimentManager.default_vec_env_cls,\n",
        "        )\n",
        "\n",
        "        print(f\"Environment observation space: {env.observation_space}\")\n",
        "        print(f\"Environment action space: {env.action_space}\")\n",
        "\n",
        "        # Load model exactly like RL Zoo3\n",
        "        kwargs = dict(seed=seed)\n",
        "\n",
        "        # Off-policy algorithm handling (like RL Zoo3)\n",
        "        off_policy_algos = [\"qrdqn\", \"dqn\", \"ddpg\", \"sac\", \"her\", \"td3\", \"tqc\"]\n",
        "        if algo in off_policy_algos:\n",
        "            kwargs.update(dict(buffer_size=1))\n",
        "            if \"optimize_memory_usage\" in hyperparams:\n",
        "                kwargs.update(optimize_memory_usage=False)\n",
        "\n",
        "        # Custom objects (like RL Zoo3)\n",
        "        custom_objects = {\n",
        "            \"learning_rate\": 0.0,\n",
        "            \"lr_schedule\": lambda _: 0.0,\n",
        "            \"clip_range\": lambda _: 0.0,\n",
        "        }\n",
        "\n",
        "        model = ALGOS[algo].load(\n",
        "            model_path,\n",
        "            custom_objects=custom_objects,\n",
        "            device='auto',\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        print(\"‚úì Model loaded successfully\")\n",
        "\n",
        "        # Recording setup\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = 30\n",
        "        all_frames = []\n",
        "\n",
        "        # Reset environment\n",
        "        obs = env.reset()\n",
        "\n",
        "        # Tracking variables (exactly like RL Zoo3)\n",
        "        episode_reward = 0.0\n",
        "        episode_rewards, episode_lengths = [], []\n",
        "        ep_len = 0\n",
        "        lstm_states = None\n",
        "        episode_start = np.ones((env.num_envs,), dtype=bool)\n",
        "\n",
        "        # Atari-specific tracking\n",
        "        atari_scores = []\n",
        "        atari_lengths = []\n",
        "\n",
        "        print(f\"\\nStarting recording for {n_timesteps} timesteps...\")\n",
        "        print(\"Deterministic actions (like RL Zoo3 for Atari)\")\n",
        "\n",
        "        for timestep in range(n_timesteps):\n",
        "            # Get action exactly like RL Zoo3\n",
        "            action, lstm_states = model.predict(\n",
        "                obs,\n",
        "                state=lstm_states,\n",
        "                episode_start=episode_start,\n",
        "                deterministic=True,  # Deterministic for Atari (like RL Zoo3)\n",
        "            )\n",
        "\n",
        "            # Step environment\n",
        "            obs, reward, done, infos = env.step(action)\n",
        "            episode_start = done\n",
        "\n",
        "            # Capture frame for video\n",
        "            try:\n",
        "                frame = env.render('rgb_array')\n",
        "                if frame is not None:\n",
        "                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    all_frames.append(frame_bgr)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Track rewards exactly like RL Zoo3\n",
        "            episode_reward += reward[0]\n",
        "            ep_len += 1\n",
        "\n",
        "            # CRITICAL: Atari-specific handling (like RL Zoo3)\n",
        "            if env.num_envs == 1:\n",
        "                if is_atari and infos is not None:\n",
        "                    episode_infos = infos[0].get(\"episode\")\n",
        "                    if episode_infos is not None:\n",
        "                        # This is the TRUE Atari score that RL Zoo3 prints!\n",
        "                        atari_score = episode_infos['r']\n",
        "                        atari_length = episode_infos['l']\n",
        "\n",
        "                        print(f\"Atari Episode Score: {atari_score:.2f}\")\n",
        "                        print(f\"Atari Episode Length: {atari_length}\")\n",
        "\n",
        "                        atari_scores.append(atari_score)\n",
        "                        atari_lengths.append(atari_length)\n",
        "\n",
        "                        # Reset counters\n",
        "                        episode_reward = 0.0\n",
        "                        ep_len = 0\n",
        "\n",
        "                elif done and not is_atari:\n",
        "                    print(f\"Episode Reward: {episode_reward:.2f}\")\n",
        "                    print(f\"Episode Length: {ep_len}\")\n",
        "                    episode_rewards.append(episode_reward)\n",
        "                    episode_lengths.append(ep_len)\n",
        "                    episode_reward = 0.0\n",
        "                    ep_len = 0\n",
        "\n",
        "            # Progress indicator\n",
        "            if timestep % 1000 == 0:\n",
        "                print(f\"Progress: {timestep}/{n_timesteps} timesteps\")\n",
        "\n",
        "        # Final summary\n",
        "        print(f\"\\n=== RECORDING SUMMARY ===\")\n",
        "        if atari_scores:\n",
        "            print(f\"Atari Episodes Recorded: {len(atari_scores)}\")\n",
        "            print(f\"Atari Scores: {atari_scores}\")\n",
        "            print(f\"Atari Lengths: {atari_lengths}\")\n",
        "            print(f\"Mean Atari Score: {np.mean(atari_scores):.2f} +/- {np.std(atari_scores):.2f}\")\n",
        "            print(f\"Mean Atari Length: {np.mean(atari_lengths):.2f} +/- {np.std(atari_lengths):.2f}\")\n",
        "\n",
        "        # Write video\n",
        "        if all_frames:\n",
        "            print(f\"\\nSaving video with {len(all_frames)} frames...\")\n",
        "            h, w = all_frames[0].shape[:2]\n",
        "            out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "            for i, frame in enumerate(all_frames):\n",
        "                out.write(frame)\n",
        "                if i % 1000 == 0:\n",
        "                    print(f\"Writing progress: {i}/{len(all_frames)} frames\")\n",
        "\n",
        "            out.release()\n",
        "            print(f\"‚úÖ Video saved: {output_path}\")\n",
        "\n",
        "            # File info\n",
        "            file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "            duration = len(all_frames) / fps\n",
        "            print(f\"üìÅ Size: {file_size:.1f} MB\")\n",
        "            print(f\"‚è±Ô∏è  Duration: {duration:.1f} seconds\")\n",
        "            print(f\"üéÆ Episodes in video: {len(atari_scores)}\")\n",
        "        else:\n",
        "            print(\"‚ùå No frames captured!\")\n",
        "\n",
        "        env.close()\n",
        "        return True\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"RL Zoo3 not available: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error in RL Zoo3 replication: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def simple_comparison_test(model_path, env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Quick test to compare reward types\n",
        "    \"\"\"\n",
        "    print(\"=== QUICK REWARD COMPARISON TEST ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS, create_test_env, get_saved_hyperparams\n",
        "        from rl_zoo3.exp_manager import ExperimentManager\n",
        "\n",
        "        # Create environment like RL Zoo3\n",
        "        model_dir = os.path.dirname(model_path)\n",
        "        log_path = os.path.dirname(model_dir)\n",
        "        stats_path = os.path.join(log_path, env_id)\n",
        "        hyperparams, maybe_stats_path = get_saved_hyperparams(stats_path, norm_reward=False, test_mode=True)\n",
        "\n",
        "        env = create_test_env(\n",
        "            env_id, n_envs=1, stats_path=maybe_stats_path, seed=0, log_dir=None,\n",
        "            should_render=False, hyperparams=hyperparams, env_kwargs={},\n",
        "            vec_env_cls=ExperimentManager.default_vec_env_cls,\n",
        "        )\n",
        "\n",
        "        model = ALGOS['dqn'].load(model_path, buffer_size=1)\n",
        "\n",
        "        obs = env.reset()\n",
        "        step_count = 0\n",
        "        episode_reward = 0\n",
        "\n",
        "        print(\"Running quick test to compare reward values...\")\n",
        "\n",
        "        for _ in range(1000):\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, infos = env.step(action)\n",
        "\n",
        "            episode_reward += reward[0]\n",
        "            step_count += 1\n",
        "\n",
        "            if infos and infos[0]:\n",
        "                if 'episode' in infos[0]:\n",
        "                    episode_info = infos[0]['episode']\n",
        "                    print(f\"\\nüéØ FOUND THE DIFFERENCE!\")\n",
        "                    print(f\"   Wrapper reward sum: {episode_reward:.2f}\")\n",
        "                    print(f\"   True Atari score: {episode_info['r']:.2f}\")\n",
        "                    print(f\"   Episode length: {episode_info['l']}\")\n",
        "                    print(f\"   Steps taken: {step_count}\")\n",
        "                    break\n",
        "                elif abs(reward[0]) > 0:\n",
        "                    print(f\"Step {step_count}: reward={reward[0]:.1f}, info={infos[0]}\")\n",
        "\n",
        "        env.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Comparison test failed: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(\"Running quick comparison test first...\")\n",
        "        simple_comparison_test(MODEL_PATH)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Now recording with exact RL Zoo3 matching...\")\n",
        "\n",
        "        success = record_exactly_like_rl_zoo3(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"exact_rl_zoo3_spaceinvaders.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            n_timesteps=5000\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            print(\"\\nüéâ SUCCESS! This should now match RL Zoo3 exactly!\")\n",
        "            print(\"The video shows the same episodes that RL Zoo3 evaluates.\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Failed to replicate RL Zoo3 setup.\")\n",
        "    else:\n",
        "        print(f\"Model not found: {MODEL_PATH}\")"
      ],
      "metadata": {
        "id": "kUR1qmxO_Wj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# diagnostically debugging the episode for completion while rendering all steps for comparison\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "def create_model_compatible_env(env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Create environment that matches the model's expected input format\n",
        "    \"\"\"\n",
        "    import gymnasium as gym\n",
        "\n",
        "    def make_env():\n",
        "        env = gym.make(env_id, render_mode='rgb_array')\n",
        "        env = AtariWrapper(env)  # This gives us (84, 84, 1) grayscale\n",
        "        return env\n",
        "\n",
        "    # Create vectorized environment with frame stacking\n",
        "    vec_env = DummyVecEnv([make_env])\n",
        "    vec_env = VecFrameStack(vec_env, n_stack=4)  # This gives us (4, 84, 84)\n",
        "\n",
        "    return vec_env\n",
        "\n",
        "def record_with_proper_atari_tracking(model_path, output_path=\"fixed_atari_recording.mp4\",\n",
        "                                    env_id='SpaceInvadersNoFrameskip-v4', n_timesteps=5000):\n",
        "    \"\"\"\n",
        "    Record with proper Atari reward tracking but compatible observations\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== HYBRID APPROACH: RL Zoo3 Rewards + Compatible Observations ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "\n",
        "        # Create model-compatible environment (preprocessed)\n",
        "        model_env = create_model_compatible_env(env_id)\n",
        "        print(f\"Model environment obs space: {model_env.observation_space}\")\n",
        "\n",
        "        # Create separate raw environment for getting true Atari scores\n",
        "        import gymnasium as gym\n",
        "        raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "        print(f\"Raw environment obs space: {raw_env.observation_space}\")\n",
        "\n",
        "        # Load model with compatible environment\n",
        "        print(f\"Loading model: {model_path}\")\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "        print(\"‚úì Model loaded successfully\")\n",
        "\n",
        "        # Video recording setup\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = 30\n",
        "        all_frames = []\n",
        "\n",
        "        # Reset both environments\n",
        "        model_obs = model_env.reset()\n",
        "        raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "        print(f\"Model obs shape: {model_obs.shape}\")\n",
        "        print(f\"Raw obs shape: {raw_obs.shape}\")\n",
        "\n",
        "        # Tracking variables (like RL Zoo3)\n",
        "        episode_reward = 0.0  # Wrapper reward sum\n",
        "        step_count = 0\n",
        "        atari_scores = []\n",
        "        atari_lengths = []\n",
        "\n",
        "        print(f\"\\nStarting synchronized recording for {n_timesteps} timesteps...\")\n",
        "\n",
        "        for timestep in range(n_timesteps):\n",
        "            # Get action from model using preprocessed observations\n",
        "            action, _ = model.predict(model_obs, deterministic=True)\n",
        "\n",
        "            # Step both environments with same action\n",
        "            model_obs, model_reward, model_done, model_info = model_env.step(action)\n",
        "\n",
        "            # Extract scalar action for raw environment\n",
        "            raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "            raw_obs, raw_reward, raw_term, raw_trunc, raw_info = raw_env.step(raw_action)\n",
        "\n",
        "            # Track wrapper reward (like RL Zoo3 does internally)\n",
        "            episode_reward += model_reward[0]\n",
        "            step_count += 1\n",
        "\n",
        "            # Capture frame from raw environment (high resolution)\n",
        "            try:\n",
        "                frame = raw_env.render()\n",
        "                if frame is not None:\n",
        "                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    all_frames.append(frame_bgr)\n",
        "            except Exception as e:\n",
        "                if timestep < 5:\n",
        "                    print(f\"Frame capture warning: {e}\")\n",
        "\n",
        "            # CRITICAL: Check for Atari episode completion in raw_info\n",
        "            if raw_info and 'episode' in raw_info:\n",
        "                episode_info = raw_info['episode']\n",
        "                atari_score = episode_info['r']\n",
        "                atari_length = episode_info['l']\n",
        "\n",
        "                print(f\"üéÆ Atari Episode Score: {atari_score:.2f}\")\n",
        "                print(f\"üéÆ Atari Episode Length: {atari_length}\")\n",
        "                print(f\"   Wrapper reward sum: {episode_reward:.2f}\")\n",
        "                print(f\"   Steps in this recording: {step_count}\")\n",
        "\n",
        "                atari_scores.append(atari_score)\n",
        "                atari_lengths.append(atari_length)\n",
        "\n",
        "                # Reset tracking\n",
        "                episode_reward = 0.0\n",
        "                step_count = 0\n",
        "\n",
        "            # Progress indicator\n",
        "            if timestep % 1000 == 0 and timestep > 0:\n",
        "                print(f\"Progress: {timestep}/{n_timesteps} timesteps, \"\n",
        "                      f\"Episodes: {len(atari_scores)}, \"\n",
        "                      f\"Frames: {len(all_frames)}\")\n",
        "\n",
        "            # Check if we should stop (got enough episodes or model environment is done)\n",
        "            if model_done[0]:\n",
        "                print(f\"Model environment signaled done at timestep {timestep}\")\n",
        "                # Reset model environment but continue\n",
        "                model_obs = model_env.reset()\n",
        "\n",
        "        # Final results\n",
        "        print(f\"\\n=== RECORDING RESULTS ===\")\n",
        "        if atari_scores:\n",
        "            print(f\"‚úÖ Atari Episodes: {len(atari_scores)}\")\n",
        "            print(f\"üìä Scores: {atari_scores}\")\n",
        "            print(f\"üìè Lengths: {atari_lengths}\")\n",
        "            print(f\"üéØ Mean Score: {np.mean(atari_scores):.2f} ¬± {np.std(atari_scores):.2f}\")\n",
        "            print(f\"üìê Mean Length: {np.mean(atari_lengths):.2f} ¬± {np.std(atari_lengths):.2f}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No complete Atari episodes detected!\")\n",
        "            print(f\"   Total timesteps: {timestep + 1}\")\n",
        "            print(f\"   Current wrapper reward: {episode_reward:.2f}\")\n",
        "            print(\"   Try increasing n_timesteps or check episode detection\")\n",
        "\n",
        "        # Save video\n",
        "        if all_frames:\n",
        "            print(f\"\\nüé• Saving video with {len(all_frames)} frames...\")\n",
        "            h, w = all_frames[0].shape[:2]\n",
        "            out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "            for i, frame in enumerate(all_frames):\n",
        "                out.write(frame)\n",
        "                if i % 1500 == 0:  # Progress every 50 seconds at 30fps\n",
        "                    print(f\"   Writing: {i}/{len(all_frames)} frames ({100*i/len(all_frames):.1f}%)\")\n",
        "\n",
        "            out.release()\n",
        "\n",
        "            # File statistics\n",
        "            file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "            duration = len(all_frames) / fps\n",
        "            print(f\"‚úÖ Video saved: {output_path}\")\n",
        "            print(f\"üìÅ Size: {file_size:.1f} MB\")\n",
        "            print(f\"‚è±Ô∏è  Duration: {duration:.1f} seconds\")\n",
        "\n",
        "            if atari_scores:\n",
        "                episodes_per_minute = len(atari_scores) / (duration / 60)\n",
        "                print(f\"üéÆ Episodes/minute: {episodes_per_minute:.1f}\")\n",
        "        else:\n",
        "            print(\"‚ùå No frames captured for video!\")\n",
        "\n",
        "        # Cleanup\n",
        "        model_env.close()\n",
        "        raw_env.close()\n",
        "\n",
        "        return len(atari_scores) > 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in hybrid recording: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def debug_episode_detection(model_path, env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Debug why episodes might not be detected properly\n",
        "    \"\"\"\n",
        "    print(\"=== DEBUGGING EPISODE DETECTION ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "        import gymnasium as gym\n",
        "\n",
        "        # Create environments\n",
        "        model_env = create_model_compatible_env(env_id)\n",
        "        raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        # Load model\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "\n",
        "        # Reset\n",
        "        model_obs = model_env.reset()\n",
        "        raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "        print(f\"Initial raw_info: {raw_info}\")\n",
        "\n",
        "        episode_reward = 0\n",
        "        wrapper_reward = 0\n",
        "\n",
        "        for step in range(2000):  # Limit for debugging\n",
        "            # Get action and step both environments\n",
        "            action, _ = model.predict(model_obs, deterministic=True)\n",
        "            model_obs, m_reward, m_done, m_info = model_env.step(action)\n",
        "\n",
        "            raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "            raw_obs, r_reward, r_term, r_trunc, r_info = raw_env.step(raw_action)\n",
        "\n",
        "            episode_reward += r_reward\n",
        "            wrapper_reward += m_reward[0]\n",
        "\n",
        "            # Log interesting events\n",
        "            if (step < 10 or\n",
        "                step % 100 == 0 or\n",
        "                abs(r_reward) > 0 or\n",
        "                r_info or\n",
        "                m_done[0] or\n",
        "                r_term or r_trunc):\n",
        "\n",
        "                print(f\"Step {step:4d}: raw_r={r_reward:5.1f}, wrap_r={m_reward[0]:5.1f}, \"\n",
        "                      f\"done={m_done[0]}, term={r_term}, trunc={r_trunc}\")\n",
        "\n",
        "                if r_info:\n",
        "                    print(f\"          raw_info: {r_info}\")\n",
        "                if m_info:\n",
        "                    print(f\"          model_info: {m_info}\")\n",
        "\n",
        "                # Check for episode completion\n",
        "                if r_info and 'episode' in r_info:\n",
        "                    episode_info = r_info['episode']\n",
        "                    print(f\"üéØ EPISODE COMPLETE!\")\n",
        "                    print(f\"   True Atari Score: {episode_info['r']:.2f}\")\n",
        "                    print(f\"   Episode Length: {episode_info['l']}\")\n",
        "                    print(f\"   Raw reward sum: {episode_reward:.2f}\")\n",
        "                    print(f\"   Wrapper reward sum: {wrapper_reward:.2f}\")\n",
        "                    break\n",
        "\n",
        "            if m_done[0]:\n",
        "                print(f\"Model environment done at step {step}\")\n",
        "                break\n",
        "\n",
        "        model_env.close()\n",
        "        raw_env.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Debug failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(\"First, let's debug episode detection...\")\n",
        "        debug_episode_detection(MODEL_PATH)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Now recording with hybrid approach...\")\n",
        "\n",
        "        success = record_with_proper_atari_tracking(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"hybrid_atari_recording.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            n_timesteps=10000  # Increased to capture more episodes\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            print(\"\\nüéâ SUCCESS! Video should now show proper Atari episodes!\")\n",
        "            print(\"This matches RL Zoo3's evaluation but with working observations.\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Recording had issues. Check the debug output above.\")\n",
        "    else:\n",
        "        print(f\"Model not found: {MODEL_PATH}\")\n",
        "        print(\"Please update MODEL_PATH with the correct path.\")"
      ],
      "metadata": {
        "id": "Ww0CsDi9_8yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make use of the 'lives' info in the r_info info dict, not the nonexistant 'episode' dict\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "def record_with_life_episode_detection(model_path, output_path=\"life_based_recording.mp4\",\n",
        "                                     env_id='SpaceInvadersNoFrameskip-v4', max_episodes=3):\n",
        "    \"\"\"\n",
        "    Record Atari episodes using life-based episode detection and manual score tracking\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== LIFE-BASED EPISODE DETECTION ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "\n",
        "        # Create model-compatible environment (preprocessed)\n",
        "        def make_env():\n",
        "            env = gym.make(env_id, render_mode='rgb_array')\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        model_env = DummyVecEnv([make_env])\n",
        "        model_env = VecFrameStack(model_env, n_stack=4)\n",
        "\n",
        "        # Create separate raw environment for rendering and score tracking\n",
        "        raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        print(f\"Model env obs space: {model_env.observation_space}\")\n",
        "        print(f\"Raw env obs space: {raw_env.observation_space}\")\n",
        "\n",
        "        # Load model\n",
        "        print(f\"Loading model: {model_path}\")\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "        print(\"‚úì Model loaded successfully\")\n",
        "\n",
        "        # Video recording setup\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = 60\n",
        "        all_frames = []\n",
        "\n",
        "        # Episode tracking variables\n",
        "        episodes_completed = 0\n",
        "        episode_scores = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        while episodes_completed < max_episodes:\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"STARTING EPISODE {episodes_completed + 1}/{max_episodes}\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            # Reset both environments\n",
        "            model_obs = model_env.reset()\n",
        "            raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "            # Episode state tracking\n",
        "            current_lives = raw_info.get('lives', 3)\n",
        "            initial_lives = current_lives\n",
        "            episode_score = 0\n",
        "            episode_steps = 0\n",
        "            life_lost_recently = False\n",
        "\n",
        "            print(f\"Starting with {current_lives} lives\")\n",
        "\n",
        "            # Play until all lives are lost\n",
        "            while current_lives > 0:\n",
        "                # Get action from model\n",
        "                action, _ = model.predict(model_obs, deterministic=True)\n",
        "\n",
        "                # Step both environments\n",
        "                model_obs, model_reward, model_done, model_info = model_env.step(action)\n",
        "\n",
        "                raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "                raw_obs, raw_reward, raw_term, raw_trunc, raw_info = raw_env.step(raw_action)\n",
        "\n",
        "                # Track score and steps\n",
        "                episode_score += raw_reward\n",
        "                episode_steps += 1\n",
        "\n",
        "                # Capture frame\n",
        "                try:\n",
        "                    frame = raw_env.render()\n",
        "                    if frame is not None:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                        all_frames.append(frame_bgr)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                # Check for life loss\n",
        "                new_lives = raw_info.get('lives', current_lives)\n",
        "                if new_lives < current_lives:\n",
        "                    print(f\"  Life lost! Lives: {current_lives} ‚Üí {new_lives}, Score: {episode_score}, Steps: {episode_steps}\")\n",
        "                    current_lives = new_lives\n",
        "                    life_lost_recently = True\n",
        "\n",
        "                # Check if model environment reset (happens on life loss in wrapped env)\n",
        "                if model_done[0]:\n",
        "                    print(f\"  Model environment reset at step {episode_steps}\")\n",
        "                    model_obs = model_env.reset()\n",
        "\n",
        "                # Progress indicator\n",
        "                if episode_steps % 500 == 0:\n",
        "                    print(f\"  Progress: {episode_steps} steps, Score: {episode_score:.0f}, Lives: {current_lives}\")\n",
        "\n",
        "                # Safety break for very long episodes\n",
        "                if episode_steps > 20000:\n",
        "                    print(f\"  Episode exceeded 20000 steps, ending...\")\n",
        "                    break\n",
        "\n",
        "            # Episode completed (all lives lost)\n",
        "            episode_scores.append(episode_score)\n",
        "            episode_lengths.append(episode_steps)\n",
        "            episodes_completed += 1\n",
        "\n",
        "            print(f\"\\nüéÆ EPISODE {episodes_completed} COMPLETED!\")\n",
        "            print(f\"   Final Score: {episode_score:.0f}\")\n",
        "            print(f\"   Episode Length: {episode_steps} steps\")\n",
        "            print(f\"   Lives Used: {initial_lives}\")\n",
        "\n",
        "            # Add a few seconds of the final screen\n",
        "            print(\"   Recording final screen for 3 seconds...\")\n",
        "            for _ in range(90):  # 3 seconds at 30fps\n",
        "                try:\n",
        "                    frame = raw_env.render()\n",
        "                    if frame is not None:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                        all_frames.append(frame_bgr)\n",
        "                except:\n",
        "                    break\n",
        "\n",
        "        # Final results\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"RECORDING SUMMARY\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"‚úÖ Episodes Completed: {episodes_completed}\")\n",
        "        print(f\"üìä Episode Scores: {episode_scores}\")\n",
        "        print(f\"üìè Episode Lengths: {episode_lengths}\")\n",
        "        print(f\"üéØ Mean Score: {np.mean(episode_scores):.1f} ¬± {np.std(episode_scores):.1f}\")\n",
        "        print(f\"üìê Mean Length: {np.mean(episode_lengths):.1f} ¬± {np.std(episode_lengths):.1f}\")\n",
        "        print(f\"üé¨ Total Frames: {len(all_frames)}\")\n",
        "\n",
        "        # Compare with RL Zoo3 results\n",
        "        print(f\"\\nüìà COMPARISON WITH RL ZOO3:\")\n",
        "        print(f\"   RL Zoo3 reported: 275-600 points, 2000-5000 steps\")\n",
        "        print(f\"   This recording: {min(episode_scores):.0f}-{max(episode_scores):.0f} points, {min(episode_lengths)}-{max(episode_lengths)} steps\")\n",
        "\n",
        "        # Save video\n",
        "        if all_frames:\n",
        "            print(f\"\\nüé• SAVING VIDEO...\")\n",
        "            h, w = all_frames[0].shape[:2]\n",
        "            out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "            for i, frame in enumerate(all_frames):\n",
        "                out.write(frame)\n",
        "                if i % 1500 == 0:\n",
        "                    print(f\"   Writing: {i}/{len(all_frames)} frames ({100*i/len(all_frames):.1f}%)\")\n",
        "\n",
        "            out.release()\n",
        "\n",
        "            # File statistics\n",
        "            file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "            duration = len(all_frames) / fps\n",
        "            print(f\"‚úÖ Video saved: {output_path}\")\n",
        "            print(f\"üìÅ Size: {file_size:.1f} MB\")\n",
        "            print(f\"‚è±Ô∏è  Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
        "            print(f\"üéÆ Episodes per minute: {episodes_completed / (duration/60):.1f}\")\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå No frames captured!\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in life-based recording: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        try:\n",
        "            model_env.close()\n",
        "            raw_env.close()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "def quick_life_test(model_path, env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Quick test to understand the life mechanics\n",
        "    \"\"\"\n",
        "    print(\"=== QUICK LIFE MECHANICS TEST ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "\n",
        "        # Create environments\n",
        "        def make_env():\n",
        "            env = gym.make(env_id, render_mode='rgb_array')\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        model_env = DummyVecEnv([make_env])\n",
        "        model_env = VecFrameStack(model_env, n_stack=4)\n",
        "        raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        # Load model\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "\n",
        "        # Reset and test\n",
        "        model_obs = model_env.reset()\n",
        "        raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "        print(f\"Initial raw_info: {raw_info}\")\n",
        "\n",
        "        current_lives = raw_info.get('lives', 3)\n",
        "        total_score = 0\n",
        "        step_count = 0\n",
        "\n",
        "        print(f\"Starting with {current_lives} lives\")\n",
        "\n",
        "        # Run until we see some life changes\n",
        "        life_changes = 0\n",
        "        while life_changes < 2 and step_count < 5000:\n",
        "            action, _ = model.predict(model_obs, deterministic=True)\n",
        "\n",
        "            model_obs, m_reward, m_done, m_info = model_env.step(action)\n",
        "            raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "            raw_obs, r_reward, r_term, r_trunc, r_info = raw_env.step(raw_action)\n",
        "\n",
        "            total_score += r_reward\n",
        "            step_count += 1\n",
        "\n",
        "            # Check for life changes\n",
        "            new_lives = r_info.get('lives', current_lives)\n",
        "            if new_lives != current_lives:\n",
        "                life_changes += 1\n",
        "                print(f\"\\nüîÑ LIFE CHANGE #{life_changes} at step {step_count}:\")\n",
        "                print(f\"   Lives: {current_lives} ‚Üí {new_lives}\")\n",
        "                print(f\"   Score so far: {total_score:.0f}\")\n",
        "                print(f\"   Model done: {m_done[0]}\")\n",
        "                print(f\"   Raw info: {r_info}\")\n",
        "                current_lives = new_lives\n",
        "\n",
        "                if m_done[0]:\n",
        "                    print(f\"   Model environment reset\")\n",
        "                    model_obs = model_env.reset()\n",
        "\n",
        "            # Log rewards\n",
        "            if abs(r_reward) > 0:\n",
        "                print(f\"Step {step_count}: +{r_reward:.0f} points (total: {total_score:.0f})\")\n",
        "\n",
        "        print(f\"\\nTest completed:\")\n",
        "        print(f\"  Total score: {total_score:.0f}\")\n",
        "        print(f\"  Steps: {step_count}\")\n",
        "        print(f\"  Final lives: {current_lives}\")\n",
        "        print(f\"  Life changes observed: {life_changes}\")\n",
        "\n",
        "        model_env.close()\n",
        "        raw_env.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Life test failed: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(\"First, running quick life mechanics test...\")\n",
        "        quick_life_test(MODEL_PATH)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Now recording with life-based episode detection...\")\n",
        "\n",
        "        success = record_with_life_episode_detection(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"life_based_spaceinvaders.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            max_episodes=2  # Record 2 complete episodes\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            print(\"\\nüéâ SUCCESS! Video shows complete Atari episodes with proper scoring!\")\n",
        "            print(\"The scores should now match RL Zoo3's evaluation results.\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Recording failed. Check the error messages above.\")\n",
        "    else:\n",
        "        print(f\"Model not found: {MODEL_PATH}\")\n",
        "        print(\"Please update MODEL_PATH with the correct path.\")"
      ],
      "metadata": {
        "id": "4rVoTo6YAjVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yaCNWbgwD11-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}