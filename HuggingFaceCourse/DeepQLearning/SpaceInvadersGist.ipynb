{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/johngrahamreynolds/e89f9c51c827c310a9762a0615ba02f2/spaceinvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7xBVPzoXxOg"
      },
      "source": [
        "# Unit 3: Deep Q-Learning with Atari Games üëæ using RL Baselines3 Zoo\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg\" alt=\"Unit 3 Thumbnail\">\n",
        "\n",
        "In this notebook, **you'll train a Deep Q-Learning agent** playing Space Invaders using [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo), a training framework based on [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "We're using the [RL-Baselines-3 Zoo integration, a vanilla version of Deep Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) with no extensions such as Double-DQN, Dueling-DQN, and Prioritized Experience Replay.\n",
        "\n",
        "‚¨áÔ∏è Here is an example of what **you will achieve** ‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9S713biXntc"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéÆ Environments:\n",
        "\n",
        "- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n",
        "\n",
        "You can see the difference between Space Invaders versions here üëâ https://gymnasium.farama.org/environments/atari/space_invaders/#variants\n",
        "\n",
        "### üìö RL-Library:\n",
        "\n",
        "- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"
      ],
      "metadata": {
        "id": "ykJiGevCMVc5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wciHGjrFYz9m"
      },
      "source": [
        "## Objectives of this notebook üèÜ\n",
        "At the end of the notebook, you will:\n",
        "- Be able to understand deeper **how RL Baselines3 Zoo works**.\n",
        "- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook is from Deep Reinforcement Learning Course\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ],
      "metadata": {
        "id": "TsnP0rjxMn1e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw6fJHIAZd-J"
      },
      "source": [
        "In this free course, you will:\n",
        "\n",
        "- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n",
        "- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n",
        "- ü§ñ Train **agents in unique environments**\n",
        "\n",
        "And more check üìö the syllabus üëâ https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n",
        "\n",
        "\n",
        "The best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vgANIBBZg1p"
      },
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "Before diving into the notebook, you need to:\n",
        "\n",
        "üî≤ üìö **[Study Deep Q-Learning by reading Unit 3](https://huggingface.co/deep-rl-course/unit3/introduction)**  ü§ó"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ],
      "metadata": {
        "id": "7kszpGFaRVhq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR0jZtYreSI5"
      },
      "source": [
        "# Let's train a Deep Q-Learning agent playing Atari' Space Invaders üëæ and upload it to the Hub.\n",
        "\n",
        "We strongly recommend students **to use Google Colab for the hands-on exercises instead of running them on their personal computers**.\n",
        "\n",
        "By using Google Colab, **you can focus on learning and experimenting without worrying about the technical aspects of setting up your environments**.\n",
        "\n",
        "To validate this hands-on for the certification process, you need to push your trained model to the Hub and **get a result of >= 200**.\n",
        "\n",
        "To find your result, go to the leaderboard and find your model, **the result = mean_reward - std of reward**\n",
        "\n",
        "For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An advice üí°\n",
        "It's better to run this colab in a copy on your Google Drive, so that **if it timeouts** you still have the saved notebook on your Google Drive and do not need to fill everything from scratch.\n",
        "\n",
        "To do that you can either do `Ctrl + S` or `File > Save a copy in Google Drive.`\n",
        "\n",
        "Also, we're going to **train it for 90 minutes with 1M timesteps**. By typing `!nvidia-smi` will tell you what GPU you're using.\n",
        "\n",
        "And if you want to train more such 10 million steps, this will take about 9 hours, potentially resulting in Colab timing out. In that case, I recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."
      ],
      "metadata": {
        "id": "Nc8BnyVEc3Ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the GPU üí™\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ],
      "metadata": {
        "id": "PU4FVzaoM6fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ],
      "metadata": {
        "id": "KV0NyFdQM9ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "jqB9T2Kv-qmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install RL-Baselines3 Zoo and its dependencies üìö\n",
        "\n",
        "If you see `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.` **this is normal and it's not a critical error** there's a conflict of version. But the packages we need are installed."
      ],
      "metadata": {
        "id": "wS_cVefO-aYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo"
      ],
      "metadata": {
        "id": "S1A_E4z3awa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig cmake ffmpeg"
      ],
      "metadata": {
        "id": "8_MllY6Om1eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S9mJiKg6SqC"
      },
      "source": [
        "To be able to use Atari games in Gymnasium we need to install atari package. And accept-rom-license to download the rom files (games files)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]"
      ],
      "metadata": {
        "id": "NsRP-lX1_2fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a virtual display üîΩ\n",
        "\n",
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install the librairies and create and run a virtual screen üñ•"
      ],
      "metadata": {
        "id": "bTpYcVZVMzUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "BE5JWP5rQIKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPgzluo9z-u"
      },
      "source": [
        "## Train our Deep Q-Learning Agent to Play Space Invaders üëæ\n",
        "\n",
        "To train an agent with RL-Baselines3-Zoo, we just need to do two things:\n",
        "\n",
        "1. Create a hyperparameter config file that will contain our training hyperparameters called `dqn.yml`.\n",
        "\n",
        "This is a template example:\n",
        "\n",
        "```\n",
        "SpaceInvadersNoFrameskip-v4:\n",
        "  env_wrapper:\n",
        "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
        "  frame_stack: 4\n",
        "  policy: 'CnnPolicy'\n",
        "  n_timesteps: !!float 1e6\n",
        "  buffer_size: 100000\n",
        "  learning_rate: !!float 1e-4\n",
        "  batch_size: 32\n",
        "  learning_starts: 100000\n",
        "  target_update_interval: 1000\n",
        "  train_freq: 4\n",
        "  gradient_steps: 1\n",
        "  exploration_fraction: 0.1\n",
        "  exploration_final_eps: 0.01\n",
        "  # If True, you need to deactivate handle_timeout_termination\n",
        "  # in the replay_buffer_kwargs\n",
        "  optimize_memory_usage: False\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VjblFSVDQOj"
      },
      "source": [
        "Here we see that:\n",
        "- We use the `Atari Wrapper` that preprocess the input (Frame reduction ,grayscale, stack 4 frames)\n",
        "- We use `CnnPolicy`, since we use Convolutional layers to process the frames\n",
        "- We train it for 10 million `n_timesteps`\n",
        "- Memory (Experience Replay) size is 100000, aka the amount of experience steps you saved to train again your agent with.\n",
        "\n",
        "üí° My advice is to **reduce the training timesteps to 1M,** which will take about 90 minutes on a P100. `!nvidia-smi` will tell you what GPU you're using. At 10 million steps, this will take about 9 hours, which could likely result in Colab timing out. I recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qTkbWrkECOJ"
      },
      "source": [
        "In terms of hyperparameters optimization, my advice is to focus on these 3 hyperparameters:\n",
        "- `learning_rate`\n",
        "- `buffer_size (Experience Memory size)`\n",
        "- `batch_size`\n",
        "\n",
        "As a good practice, you need to **check the documentation to understand what each hyperparameters does**: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn8bRTHvERRL"
      },
      "source": [
        "2. We start the training and save the models on `logs` folder üìÅ\n",
        "\n",
        "- Define the algorithm after `--algo`, where we save the model after `-f` and where the hyperparameter config is after `-c`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr1TVW4xfbz3"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.train --algo dqn --env SpaceInvadersNoFrameskip-v4  -f logs/  -c dqn.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLomIiMKQaf"
      },
      "source": [
        "## Let's evaluate our agent üëÄ\n",
        "- RL-Baselines3-Zoo provides `enjoy.py`, a python script to evaluate our agent. In most RL libraries, we call the evaluation script `enjoy.py`.\n",
        "- Let's evaluate it for 5000 timesteps üî•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co5um_KeKbBJ"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liBeTltiHJtr"
      },
      "source": [
        "## Publish our trained model on the Hub üöÄ\n",
        "Now that we saw we got good results after the training, we can publish our trained model on the hub ü§ó with one line of code.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezbHS1q3HYVV"
      },
      "source": [
        "By using `rl_zoo3.push_to_hub` **you evaluate, record a replay, generate a model card of your agent and push it to the hub**.\n",
        "\n",
        "This way:\n",
        "- You can **showcase our work** üî•\n",
        "- You can **visualize your agent playing** üëÄ\n",
        "- You can **share with the community an agent that others can use** üíæ\n",
        "- You can **access a leaderboard üèÜ to see how well your agent is performing compared to your classmates** üëâ  https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMSeZRBiHk6X"
      },
      "source": [
        "To be able to share your model with the community there are three more steps to follow:\n",
        "\n",
        "1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n",
        "\n",
        "2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
        "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O6FI0F8HnzE"
      },
      "source": [
        "- Copy the token\n",
        "- Run the cell below and past the token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppu9yePwHrZX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RVEdunPHs8B"
      },
      "source": [
        "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSLwdmvhHvjw"
      },
      "source": [
        "3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW436XnhHw1H"
      },
      "source": [
        "Let's run push_to_hub.py file to upload our trained agent to the Hub.\n",
        "\n",
        "`--repo-name `: The name of the repo\n",
        "\n",
        "`-orga`: Your Hugging Face username\n",
        "\n",
        "`-f`: Where the trained model folder is (in our case `logs`)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/select-id.png\" alt=\"Select Id\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygk2sEktTDEw"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4 -orga MarioBarbeque -f logs/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /tmp && ls"
      ],
      "metadata": {
        "id": "tXyyRZwjeSi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff89kd2HL1_s"
      },
      "source": [
        "Congrats ü•≥ you've just trained and uploaded your first Deep Q-Learning agent using RL-Baselines-3 Zoo. The script above should have displayed a link to a model repository such as https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4. When you go to this link, you can:\n",
        "\n",
        "- See a **video preview of your agent** at the right.\n",
        "- Click \"Files and versions\" to see all the files in the repository.\n",
        "- Click \"Use in stable-baselines3\" to get a code snippet that shows how to load the model.\n",
        "- A model card (`README.md` file) which gives a description of the model and the hyperparameters you used.\n",
        "\n",
        "Under the hood, the Hub uses git-based repositories (don't worry if you don't know what git is), which means you can update the model with new versions as you experiment and improve your agent.\n",
        "\n",
        "**Compare the results of your agents with your classmates** using the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) üèÜ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyRKcCYY-dIo"
      },
      "source": [
        "## Load a powerful trained model üî•\n",
        "- The Stable-Baselines3 team uploaded **more than 150 trained Deep Reinforcement Learning agents on the Hub**.\n",
        "\n",
        "You can find them here: üëâ https://huggingface.co/sb3\n",
        "\n",
        "Some examples:\n",
        "- Asteroids: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n",
        "- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n",
        "- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n",
        "- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n",
        "\n",
        "Let's load an agent playing Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-9QVFIROI5Y"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZQNY_r6NJtC"
      },
      "source": [
        "1. We download the model using `rl_zoo3.load_from_hub`, and place it in a new folder that we can call `rl_trained`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdBNZHy0NGTR"
      },
      "outputs": [],
      "source": [
        "# Download model and save it into the logs/ folder\n",
        "!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFt6hmWsNdBo"
      },
      "source": [
        "2. Let's evaluate if for 5000 timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOxs0rNuN0uS"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxMDuDfPON57"
      },
      "source": [
        "Why not trying to train your own **Deep Q-Learning Agent playing BeamRiderNoFrameskip-v4? üèÜ.**\n",
        "\n",
        "If you want to try, check https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters **in the model card, you have the hyperparameters of the trained agent.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL_ZtUgpOuY6"
      },
      "source": [
        "But finding hyperparameters can be a daunting task. Fortunately, we'll see in the next Unit, how we can **use Optuna for optimizing the Hyperparameters üî•.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqaco8W-huW"
      },
      "source": [
        "## Some additional challenges üèÜ\n",
        "The best way to learn **is to try things by your own**!\n",
        "\n",
        "In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\n",
        "\n",
        "Here's a list of environments you can try to train your agent with:\n",
        "- BeamRiderNoFrameskip-v4\n",
        "- BreakoutNoFrameskip-v4\n",
        "- EnduroNoFrameskip-v4\n",
        "- PongNoFrameskip-v4\n",
        "\n",
        "Also, **if you want to learn to implement Deep Q-Learning by yourself**, you definitely should look at CleanRL implementation: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paS-XKo4-kmu"
      },
      "source": [
        "________________________________________________________________________\n",
        "Congrats on finishing this chapter!\n",
        "\n",
        "If you‚Äôre still feel confused with all these elements...it's totally normal! **This was the same for me and for all people who studied RL.**\n",
        "\n",
        "Take time to really **grasp the material before continuing and try the additional challenges**. It‚Äôs important to master these elements and having a solid foundations.\n",
        "\n",
        "In the next unit, **we‚Äôre going to learn about [Optuna](https://optuna.org/)**. One of the most critical task in Deep Reinforcement Learning is to find a good set of training hyperparameters. And Optuna is a library that helps you to automate the search.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WRx7tO7-mvC"
      },
      "source": [
        "\n",
        "\n",
        "### This is a course built with you üë∑üèø‚Äç‚ôÄÔ∏è\n",
        "\n",
        "Finally, we want to improve and update the course iteratively with your feedback. If you have some, please fill this form üëâ https://forms.gle/3HgA7bEHwAmmLfwh9\n",
        "\n",
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See you on Bonus unit 2! üî•"
      ],
      "metadata": {
        "id": "Kc3udPT-RcXc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS3Xerx0fIMV"
      },
      "source": [
        "## Custom code to render an mp4 sample of our agent playing\n",
        "\n",
        "### We encounter a variety of errors in this process, so we iterively debug the with the aid of Anthropic's Claude"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model from HF\n",
        "!python -m rl_zoo3.load_from_hub --algo dqn --env SpaceInvadersNoFrameskip-v4 -orga MarioBarbeque -f logs/"
      ],
      "metadata": {
        "id": "0iMLMQZGpDZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ale-py[roms] # nonexistant, must use the below\n",
        "!pip install gymnasium[atari]\n",
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "tKU6SDkjoV-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autorom[accept-rom-license]\n",
        "# Then import to download ROMs\n",
        "!python -c \"import ale_py.roms\""
      ],
      "metadata": {
        "id": "0_nkXDjyoXkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "print(gym.envs.registry.keys())\n",
        "# Look for Space Invaders variants, nonexistant but we have it in the ale_py.roms module"
      ],
      "metadata": {
        "id": "qnyMq9t-ofi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "metadata": {
        "id": "sQ-rSZ-SqtP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# register extended ALE environments in gym\n",
        "import ale_py\n",
        "import gymnasium as gym\n",
        "gym.register_envs(ale_py)"
      ],
      "metadata": {
        "id": "jhmXEj-G2yS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial rendering\n",
        "\n",
        "Here the rendering isnt on par with the expected output - there seems to be a discrepancy between when the rendered video's episode ends and when one canonically expects the episode of a game of Space Invaders to end.\n",
        "\n",
        "In this rendering attempt we are using SB3's AtariWrapper and a VecEnv to stack frames in a wrapped Atari environment manually."
      ],
      "metadata": {
        "id": "f2sHrSkrNL7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "def make_atari_env(env_id='SpaceInvadersNoFrameskip-v4', render_mode='rgb_array'):\n",
        "    \"\"\"\n",
        "    Create Atari environment with the same preprocessing as used in training\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        env = gym.make(env_id, render_mode=render_mode)\n",
        "        env = AtariWrapper(env)  # This applies the standard Atari preprocessing\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "def record_preprocessed_agent(model_path, output_path=\"enjoy.mp4\",\n",
        "                            env_id='SpaceInvadersNoFrameskip-v4', n_episodes=1):\n",
        "    \"\"\"\n",
        "    Record agent with proper Atari preprocessing pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Creating environment: {env_id}\")\n",
        "\n",
        "    # Create vectorized environment with preprocessing (for model)\n",
        "    env = DummyVecEnv([make_atari_env(env_id, render_mode='rgb_array')])\n",
        "    env = VecFrameStack(env, n_stack=4)  # Stack 4 frames as expected by model\n",
        "\n",
        "    # Create separate environment for rendering (raw frames)\n",
        "    render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    model = DQN.load(model_path, env=env)\n",
        "    print(\"Model loaded successfully\")\n",
        "\n",
        "    # Recording setup\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = 60\n",
        "    all_frames = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        # Reset both environments\n",
        "        obs = env.reset()\n",
        "        render_obs, render_info = render_env.reset()\n",
        "\n",
        "        done = False\n",
        "        episode_frames = []\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        print(f\"\\nRecording episode {episode + 1}/{n_episodes}...\")\n",
        "        print(f\"Model input shape: {obs.shape}\")\n",
        "        print(f\"Render frame shape: {render_obs.shape}\")\n",
        "\n",
        "        while not done:\n",
        "            # Get action from model using preprocessed observations\n",
        "            action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "            # Step both environments with the same action\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            render_obs, render_reward, render_terminated, render_truncated, render_info = render_env.step(action[0])\n",
        "\n",
        "            episode_reward += reward[0]\n",
        "            step_count += 1\n",
        "\n",
        "            # Capture high-resolution frame for video\n",
        "            try:\n",
        "                frame = render_env.render()\n",
        "                if frame is not None and len(frame.shape) == 3:\n",
        "                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    episode_frames.append(frame_bgr)\n",
        "            except Exception as e:\n",
        "                if step_count == 1:\n",
        "                    print(f\"Warning: Could not capture frame: {e}\")\n",
        "\n",
        "            # Safety break for very long episodes\n",
        "            if step_count > 50000:\n",
        "                print(\"Episode exceeded 50000 steps, ending...\")\n",
        "                break\n",
        "\n",
        "        all_frames.extend(episode_frames)\n",
        "        print(f\"Episode {episode + 1} completed:\")\n",
        "        print(f\"  - Steps: {step_count}\")\n",
        "        print(f\"  - Total Reward: {episode_reward:.2f}\")\n",
        "        print(f\"  - Frames captured: {len(episode_frames)}\")\n",
        "\n",
        "    # Write video\n",
        "    if all_frames:\n",
        "        h, w = all_frames[0].shape[:2]\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "        print(f\"\\nWriting {len(all_frames)} frames to {output_path}...\")\n",
        "        for i, frame in enumerate(all_frames):\n",
        "            out.write(frame)\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Progress: {i}/{len(all_frames)} frames\")\n",
        "\n",
        "        out.release()\n",
        "        print(f\"‚úì Video saved to {output_path}\")\n",
        "    else:\n",
        "        print(\"‚úó No frames captured!\")\n",
        "\n",
        "    env.close()\n",
        "    render_env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual model path\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        record_preprocessed_agent(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"enjoy.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            n_episodes=1\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Model file not found: {MODEL_PATH}\")\n",
        "        print(\"Please update MODEL_PATH with the correct path to your .zip file\")\n",
        "\n",
        "        # List files in logs directory to help find the model\n",
        "        if os.path.exists(\"logs\"):\n",
        "            print(\"\\nAvailable files in logs/:\")\n",
        "            for root, dirs, files in os.walk(\"logs\"):\n",
        "                for file in files:\n",
        "                    if file.endswith('.zip'):\n",
        "                        print(f\"  {os.path.join(root, file)}\")"
      ],
      "metadata": {
        "id": "TiipkO3iwNF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Attempt\n",
        "\n",
        "To address the oddity of the rendered output terminating strangely, we rather make use, with try-catch handling that defers to the manual rendering as above, of the underlying code of RL Zoo3, which is indeed the same library we actually used to train the model. One might expext that this preferred method of creating the environments, that is, via RL Zoo's standard operating functions, one might more accurately connect the rendered output to the instinctual picture."
      ],
      "metadata": {
        "id": "xp6rZUVtZHqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "def record_with_proper_env_matching(model_path, output_path=\"spaceinvaders_demo.mp4\",\n",
        "                                   env_id='SpaceInvadersNoFrameskip-v4', n_episodes=1):\n",
        "    \"\"\"\n",
        "    Record agent ensuring exact environment matching with training\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Loading model to inspect training environment: {model_path}\")\n",
        "\n",
        "    # First, load the model without any environment to inspect its expected spaces\n",
        "    model_data = DQN.load(model_path, device='cpu')\n",
        "    expected_obs_space = model_data.observation_space\n",
        "    expected_action_space = model_data.action_space\n",
        "\n",
        "    print(f\"Model expects observation space: {expected_obs_space}\")\n",
        "    print(f\"Model expects action space: {expected_action_space}\")\n",
        "\n",
        "    # Create environment that matches the model's expectations\n",
        "    def make_training_env():\n",
        "        env = gym.make(env_id, render_mode='rgb_array')\n",
        "        env = AtariWrapper(env)\n",
        "        return env\n",
        "\n",
        "    # Create vectorized environment with frame stacking\n",
        "    vec_env = DummyVecEnv([make_training_env])\n",
        "    vec_env = VecFrameStack(vec_env, n_stack=4)\n",
        "\n",
        "    print(f\"Created environment observation space: {vec_env.observation_space}\")\n",
        "    print(f\"Created environment action space: {vec_env.action_space}\")\n",
        "\n",
        "    # Verify spaces match\n",
        "    if vec_env.observation_space != expected_obs_space:\n",
        "        print(\"WARNING: Observation spaces don't match exactly!\")\n",
        "        print(f\"Expected: {expected_obs_space}\")\n",
        "        print(f\"Got: {vec_env.observation_space}\")\n",
        "        print(\"Attempting to load with force_reset=True...\")\n",
        "\n",
        "    # Load model with the matching environment\n",
        "    try:\n",
        "        model = DQN.load(model_path, env=vec_env, force_reset=True)\n",
        "        print(\"Model loaded successfully with force_reset=True\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load with environment: {e}\")\n",
        "        print(\"Loading model without environment and setting manually...\")\n",
        "        model = DQN.load(model_path)\n",
        "        model.set_env(vec_env)\n",
        "\n",
        "    # Create separate high-resolution environment for rendering\n",
        "    render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "    # Recording setup\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = 30\n",
        "    all_frames = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        # Reset both environments\n",
        "        obs = vec_env.reset()\n",
        "        render_obs, render_info = render_env.reset()\n",
        "\n",
        "        done = False\n",
        "        episode_frames = []\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        print(f\"\\nRecording episode {episode + 1}/{n_episodes}...\")\n",
        "        print(f\"Vectorized env obs shape: {obs.shape}\")\n",
        "        print(f\"Render env obs shape: {render_obs.shape}\")\n",
        "\n",
        "        while not done:\n",
        "            # Get action from model using preprocessed observations\n",
        "            action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "            # Step vectorized environment\n",
        "            obs, reward, done_vec, info = vec_env.step(action)\n",
        "            done = done_vec[0]\n",
        "\n",
        "            # Step render environment with the same action\n",
        "            render_action = action[0] if hasattr(action, '__len__') else action\n",
        "            render_obs, render_reward, render_term, render_trunc, render_info = render_env.step(render_action)\n",
        "\n",
        "            episode_reward += reward[0]\n",
        "            step_count += 1\n",
        "\n",
        "            # Capture high-resolution frame for video\n",
        "            try:\n",
        "                frame = render_env.render()\n",
        "                if frame is not None and len(frame.shape) == 3:\n",
        "                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    episode_frames.append(frame_bgr)\n",
        "            except Exception as e:\n",
        "                if step_count == 1:\n",
        "                    print(f\"Warning: Could not capture frame: {e}\")\n",
        "\n",
        "            # Safety break\n",
        "            if step_count > 10000:\n",
        "                print(\"Episode exceeded 10000 steps, ending...\")\n",
        "                break\n",
        "\n",
        "        all_frames.extend(episode_frames)\n",
        "        print(f\"Episode {episode + 1} completed:\")\n",
        "        print(f\"  - Steps: {step_count}\")\n",
        "        print(f\"  - Total Reward: {episode_reward:.2f}\")\n",
        "        print(f\"  - Frames captured: {len(episode_frames)}\")\n",
        "\n",
        "    # Write video\n",
        "    if all_frames:\n",
        "        h, w = all_frames[0].shape[:2]\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "        print(f\"\\nWriting {len(all_frames)} frames to {output_path}...\")\n",
        "        for i, frame in enumerate(all_frames):\n",
        "            out.write(frame)\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Progress: {i}/{len(all_frames)} frames\")\n",
        "\n",
        "        out.release()\n",
        "        print(f\"‚úì Video saved to {output_path}\")\n",
        "    else:\n",
        "        print(\"‚úó No frames captured!\")\n",
        "\n",
        "    vec_env.close()\n",
        "    render_env.close()\n",
        "\n",
        "def record_using_rl_zoo_config(model_path, output_path=\"spaceinvaders_demo.mp4\",\n",
        "                              env_id='SpaceInvadersNoFrameskip-v4', n_episodes=1):\n",
        "    \"\"\"\n",
        "    Use RL Zoo3's configuration files to recreate exact training environment\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "        from rl_zoo3.utils import get_saved_hyperparams\n",
        "        from stable_baselines3.common.utils import set_random_seed\n",
        "        import yaml\n",
        "\n",
        "        print(\"Attempting to use RL Zoo3 configuration...\")\n",
        "\n",
        "        # Set random seed\n",
        "        set_random_seed(0)\n",
        "\n",
        "        # Get model directory\n",
        "        model_dir = os.path.dirname(model_path)\n",
        "\n",
        "        # Try to load hyperparameters\n",
        "        try:\n",
        "            hyperparams, stats_path = get_saved_hyperparams(\n",
        "                model_dir,\n",
        "                norm_reward=False,\n",
        "                test_mode=True\n",
        "            )\n",
        "            print(f\"Loaded hyperparameters: {hyperparams}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load hyperparameters: {e}\")\n",
        "            hyperparams = {}\n",
        "            stats_path = None\n",
        "\n",
        "        # Load model without environment first\n",
        "        print(\"Loading model without environment...\")\n",
        "        model = ALGOS['dqn'].load(model_path)\n",
        "\n",
        "        # Create environment manually with Atari preprocessing\n",
        "        def make_env():\n",
        "            env = gym.make(env_id, render_mode='rgb_array')\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        vec_env = DummyVecEnv([make_env])\n",
        "        vec_env = VecFrameStack(vec_env, n_stack=4)\n",
        "\n",
        "        # Set the environment on the model\n",
        "        print(\"Setting environment on model...\")\n",
        "        model.set_env(vec_env)\n",
        "\n",
        "        # Create render environment\n",
        "        render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        # Continue with recording as before...\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = 30\n",
        "        all_frames = []\n",
        "\n",
        "        for episode in range(n_episodes):\n",
        "            obs = vec_env.reset()\n",
        "            render_obs, render_info = render_env.reset()\n",
        "\n",
        "            done = False\n",
        "            episode_frames = []\n",
        "            episode_reward = 0\n",
        "            step_count = 0\n",
        "\n",
        "            print(f\"\\nRecording episode {episode + 1}/{n_episodes}...\")\n",
        "\n",
        "            while not done:\n",
        "                action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "                obs, reward, done_vec, info = vec_env.step(action)\n",
        "                done = done_vec[0]\n",
        "\n",
        "                render_action = action[0] if hasattr(action, '__len__') else action\n",
        "                render_obs, render_reward, render_term, render_trunc, render_info = render_env.step(render_action)\n",
        "\n",
        "                episode_reward += reward[0]\n",
        "                step_count += 1\n",
        "\n",
        "                try:\n",
        "                    frame = render_env.render()\n",
        "                    if frame is not None and len(frame.shape) == 3:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                        episode_frames.append(frame_bgr)\n",
        "                except Exception as e:\n",
        "                    if step_count == 1:\n",
        "                        print(f\"Warning: Could not capture frame: {e}\")\n",
        "\n",
        "                if step_count > 10000:\n",
        "                    break\n",
        "\n",
        "            all_frames.extend(episode_frames)\n",
        "            print(f\"Episode {episode + 1} completed:\")\n",
        "            print(f\"  - Steps: {step_count}\")\n",
        "            print(f\"  - Total Reward: {episode_reward:.2f}\")\n",
        "            print(f\"  - Frames captured: {len(episode_frames)}\")\n",
        "\n",
        "        if all_frames:\n",
        "            h, w = all_frames[0].shape[:2]\n",
        "            out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "            print(f\"\\nWriting {len(all_frames)} frames to {output_path}...\")\n",
        "            for frame in all_frames:\n",
        "                out.write(frame)\n",
        "\n",
        "            out.release()\n",
        "            print(f\"‚úì Video saved to {output_path}\")\n",
        "\n",
        "        vec_env.close()\n",
        "        render_env.close()\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"RL Zoo3 not available: {e}\")\n",
        "        print(\"Falling back to manual environment creation...\")\n",
        "        record_with_proper_env_matching(model_path, output_path, env_id, n_episodes)\n",
        "    except Exception as e:\n",
        "        print(f\"RL Zoo3 approach failed: {e}\")\n",
        "        print(\"Falling back to manual environment creation...\")\n",
        "        record_with_proper_env_matching(model_path, output_path, env_id, n_episodes)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(\"Trying RL Zoo3 configuration approach...\")\n",
        "        record_using_rl_zoo_config(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"RL_Zoo3_enjoy.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            n_episodes=1\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Model file not found: {MODEL_PATH}\")\n",
        "\n",
        "        # Help find the correct path\n",
        "        if os.path.exists(\"logs\"):\n",
        "            print(\"\\nAvailable model files:\")\n",
        "            for root, dirs, files in os.walk(\"logs\"):\n",
        "                for file in files:\n",
        "                    if file.endswith('.zip'):\n",
        "                        print(f\"  {os.path.join(root, file)}\")"
      ],
      "metadata": {
        "id": "1qIbVRN703-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oddity persists\n",
        "\n",
        "The output continues to be strange, and a notably high discrepancy can be seen between the total timesteps, the total reward, and what values we found in evaluating our model with RL Zoo originally. We re-evaluate our model once more below to confirm this."
      ],
      "metadata": {
        "id": "4-W3D1JSaF13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4 --no-render --n-timesteps 5000 --folder logs/"
      ],
      "metadata": {
        "id": "HHwLr9Or31Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment comparison\n",
        "\n",
        "Here we diagnostically compare a variety of enviroment types to better understand why we have a discrepancy between the expected output of the rendered video and the RL Zoo evaluation. Furthermore, we examine some of the internal components used in the step by step evolution of the episode"
      ],
      "metadata": {
        "id": "OWEbqGE_abG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "def compare_environments(env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Compare different environment setups to understand the discrepancy\n",
        "    \"\"\"\n",
        "    print(\"=== ENVIRONMENT COMPARISON ===\")\n",
        "\n",
        "    # 1. Raw environment (no wrappers)\n",
        "    print(\"\\n1. Raw Environment:\")\n",
        "    raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "    print(f\"   Observation space: {raw_env.observation_space}\")\n",
        "    print(f\"   Action space: {raw_env.action_space}\")\n",
        "    print(f\"   Max episode steps: {raw_env.spec.max_episode_steps}\")\n",
        "    raw_env.close()\n",
        "\n",
        "    # 2. AtariWrapper only\n",
        "    print(\"\\n2. AtariWrapper Environment:\")\n",
        "    atari_env = gym.make(env_id, render_mode='rgb_array')\n",
        "    atari_env = AtariWrapper(atari_env)\n",
        "    print(f\"   Observation space: {atari_env.observation_space}\")\n",
        "    print(f\"   Action space: {atari_env.action_space}\")\n",
        "    print(f\"   Max episode steps: {getattr(atari_env.spec, 'max_episode_steps', 'Unknown')}\")\n",
        "    atari_env.close()\n",
        "\n",
        "    # 3. Vectorized + FrameStack\n",
        "    print(\"\\n3. Vectorized + FrameStack Environment:\")\n",
        "    def make_env():\n",
        "        env = gym.make(env_id, render_mode='rgb_array')\n",
        "        env = AtariWrapper(env)\n",
        "        return env\n",
        "\n",
        "    vec_env = DummyVecEnv([make_env])\n",
        "    vec_env = VecFrameStack(vec_env, n_stack=4)\n",
        "    print(f\"   Observation space: {vec_env.observation_space}\")\n",
        "    print(f\"   Action space: {vec_env.action_space}\")\n",
        "    vec_env.close()\n",
        "\n",
        "def diagnostic_record(model_path, output_path=\"diagnostic_spaceinvaders.mp4\",\n",
        "                     env_id='SpaceInvadersNoFrameskip-v4', n_episodes=3):\n",
        "    \"\"\"\n",
        "    Record with detailed diagnostics to understand the reward/episode length discrepancy\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== DIAGNOSTIC RECORDING ===\")\n",
        "    compare_environments(env_id)\n",
        "\n",
        "    # Create environments\n",
        "    def make_model_env():\n",
        "        env = gym.make(env_id, render_mode='rgb_array')\n",
        "        env = AtariWrapper(env)\n",
        "        return env\n",
        "\n",
        "    model_env = DummyVecEnv([make_model_env])\n",
        "    model_env = VecFrameStack(model_env, n_stack=4)\n",
        "\n",
        "    # Create render environment WITHOUT AtariWrapper to see raw rewards\n",
        "    render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "    # Also create an AtariWrapper render env for comparison\n",
        "    atari_render_env = gym.make(env_id, render_mode='rgb_array')\n",
        "    atari_render_env = AtariWrapper(atari_render_env)\n",
        "\n",
        "    print(f\"\\nModel environment obs space: {model_env.observation_space}\")\n",
        "    print(f\"Raw render environment obs space: {render_env.observation_space}\")\n",
        "    print(f\"AtariWrapper render environment obs space: {atari_render_env.observation_space}\")\n",
        "\n",
        "    # Load model\n",
        "    print(f\"\\nLoading model: {model_path}\")\n",
        "    model = DQN.load(model_path, env=model_env, force_reset=True)\n",
        "\n",
        "    # Video recording setup\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = 60\n",
        "    all_frames = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"EPISODE {episode + 1}/{n_episodes}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Reset all environments\n",
        "        model_obs = model_env.reset()\n",
        "        render_obs, render_info = render_env.reset()\n",
        "        atari_obs, atari_info = atari_render_env.reset()\n",
        "\n",
        "        done = False\n",
        "        episode_frames = []\n",
        "\n",
        "        # Tracking variables\n",
        "        model_reward = 0\n",
        "        raw_reward = 0\n",
        "        atari_reward = 0\n",
        "        step_count = 0\n",
        "        lives_info = []\n",
        "\n",
        "        while not done:\n",
        "            # Get action from model\n",
        "            action, _states = model.predict(model_obs, deterministic=True)\n",
        "\n",
        "            # Step model environment\n",
        "            model_obs, m_reward, done_vec, m_info = model_env.step(action)\n",
        "            done = done_vec[0]\n",
        "            model_reward += m_reward[0]\n",
        "\n",
        "            # Step raw render environment\n",
        "            render_action = action[0] if isinstance(action, np.ndarray) else action\n",
        "            render_obs, r_reward, r_term, r_trunc, r_info = render_env.step(render_action)\n",
        "            raw_reward += r_reward\n",
        "\n",
        "            # Step AtariWrapper render environment\n",
        "            atari_obs, a_reward, a_term, a_trunc, a_info = atari_render_env.step(render_action)\n",
        "            atari_reward += a_reward\n",
        "\n",
        "            step_count += 1\n",
        "\n",
        "            # Capture frame\n",
        "            frame = render_env.render()\n",
        "            if frame is not None:\n",
        "                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                episode_frames.append(frame_bgr)\n",
        "\n",
        "            # Log detailed info every 50 steps or when interesting things happen\n",
        "            if (step_count % 50 == 0 or\n",
        "                'lives' in r_info or\n",
        "                abs(r_reward) > 0 or\n",
        "                r_term or r_trunc or\n",
        "                done):\n",
        "\n",
        "                print(f\"Step {step_count:4d}:\")\n",
        "                print(f\"  Model: reward={m_reward[0]:6.1f}, done={done}, info={m_info}\")\n",
        "                print(f\"  Raw:   reward={r_reward:6.1f}, term={r_term}, trunc={r_trunc}, info={r_info}\")\n",
        "                print(f\"  Atari: reward={a_reward:6.1f}, term={a_term}, trunc={a_trunc}, info={a_info}\")\n",
        "\n",
        "                # Track lives if available\n",
        "                if 'lives' in r_info:\n",
        "                    lives_info.append((step_count, r_info['lives']))\n",
        "                    print(f\"  Lives: {r_info['lives']}\")\n",
        "\n",
        "            # Safety break\n",
        "            if step_count > 20000:\n",
        "                print(f\"Safety break at {step_count} steps\")\n",
        "                break\n",
        "\n",
        "        all_frames.extend(episode_frames)\n",
        "\n",
        "        print(f\"\\nEPISODE {episode + 1} SUMMARY:\")\n",
        "        print(f\"  Steps: {step_count}\")\n",
        "        print(f\"  Model Environment Reward: {model_reward:.2f}\")\n",
        "        print(f\"  Raw Environment Reward: {raw_reward:.2f}\")\n",
        "        print(f\"  AtariWrapper Environment Reward: {atari_reward:.2f}\")\n",
        "        print(f\"  Frames captured: {len(episode_frames)}\")\n",
        "        print(f\"  Lives info: {lives_info}\")\n",
        "        print(f\"  Final done reason: Model done={done}\")\n",
        "        print(f\"  Raw env final state: term={r_term}, trunc={r_trunc}\")\n",
        "        print(f\"  Atari env final state: term={a_term}, trunc={a_trunc}\")\n",
        "\n",
        "    # Write video\n",
        "    if all_frames:\n",
        "        print(f\"\\nWriting video with {len(all_frames)} frames...\")\n",
        "        h, w = all_frames[0].shape[:2]\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "        for frame in all_frames:\n",
        "            out.write(frame)\n",
        "\n",
        "        out.release()\n",
        "        print(f\"‚úÖ Diagnostic video saved to: {output_path}\")\n",
        "\n",
        "    # Cleanup\n",
        "    model_env.close()\n",
        "    render_env.close()\n",
        "    atari_render_env.close()\n",
        "\n",
        "def test_rl_zoo_environment_creation(model_path, env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Try to recreate the exact environment setup used by RL Zoo3\n",
        "    \"\"\"\n",
        "    print(\"\\n=== TESTING RL ZOO3 ENVIRONMENT SETUP ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3.utils import get_saved_hyperparams, create_test_env\n",
        "        from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "        model_dir = os.path.dirname(model_path)\n",
        "\n",
        "        # Get hyperparameters\n",
        "        try:\n",
        "            hyperparams, stats_path = get_saved_hyperparams(\n",
        "                model_dir,\n",
        "                norm_reward=False,\n",
        "                test_mode=True\n",
        "            )\n",
        "            print(f\"Loaded hyperparams: {hyperparams}\")\n",
        "            print(f\"Stats path: {stats_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load hyperparams: {e}\")\n",
        "            hyperparams = {}\n",
        "            stats_path = None\n",
        "\n",
        "        # Create test environment like RL Zoo3 does\n",
        "        env = create_test_env(\n",
        "            env_id,\n",
        "            n_envs=1,\n",
        "            stats_path=stats_path,\n",
        "            seed=0,\n",
        "            log_dir=None,\n",
        "            should_render=True,\n",
        "            hyperparams=hyperparams,\n",
        "            env_kwargs={}\n",
        "        )\n",
        "\n",
        "        print(f\"RL Zoo3 test env observation space: {env.observation_space}\")\n",
        "        print(f\"RL Zoo3 test env action space: {env.action_space}\")\n",
        "\n",
        "        # Test a few steps\n",
        "        obs, info = env.reset()\n",
        "        print(f\"Initial obs shape: {obs.shape}\")\n",
        "        print(f\"Initial info: {info}\")\n",
        "\n",
        "        for i in range(10):\n",
        "            action = env.action_space.sample()\n",
        "            obs, reward, terminated, truncated, info = env.step([action])\n",
        "            print(f\"Step {i+1}: reward={reward[0]:.1f}, term={terminated[0]}, trunc={truncated[0]}\")\n",
        "\n",
        "            if terminated[0] or truncated[0]:\n",
        "                break\n",
        "\n",
        "        env.close()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"RL Zoo3 not available for environment comparison\")\n",
        "    except Exception as e:\n",
        "        print(f\"RL Zoo3 environment test failed: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        # First test RL Zoo3 environment setup\n",
        "        test_rl_zoo_environment_creation(MODEL_PATH)\n",
        "\n",
        "        # Then run diagnostic recording\n",
        "        diagnostic_record(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"diagnostic_spaceinvaders.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            n_episodes=2\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Model not found: {MODEL_PATH}\")\n",
        "        print(\"Please update MODEL_PATH\")"
      ],
      "metadata": {
        "id": "bZf9mDCT4kYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m rl_zoo3.enjoy --algo dqn --env SpaceInvadersNoFrameskip-v4 --no-render --n-timesteps 10000 --folder logs/ --verbose 1"
      ],
      "metadata": {
        "id": "RW408BT17yf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Still clearly something is amiss with both the rendering and the reward\n",
        "\n",
        "We will again run the diagnostics but modify our code to effectively compare the AtariWrapper + VecFrameStack (gives a 4, 84, 84 observation space) environment consistent with the model and the raw gynmasium environment. We then compare results in both the raw and the model compatible environments"
      ],
      "metadata": {
        "id": "Rj7qOkg-d3_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# diagnostically debugging the episode for completion while rendering all steps for comparison\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "def create_model_compatible_env(env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Create environment that matches the model's expected input format\n",
        "    \"\"\"\n",
        "    import gymnasium as gym\n",
        "\n",
        "    def make_env():\n",
        "        env = gym.make(env_id, render_mode='rgb_array')\n",
        "        env = AtariWrapper(env)  # This gives us (84, 84, 1) grayscale\n",
        "        return env\n",
        "\n",
        "    # Create vectorized environment with frame stacking\n",
        "    vec_env = DummyVecEnv([make_env])\n",
        "    vec_env = VecFrameStack(vec_env, n_stack=4)  # This gives us (4, 84, 84)\n",
        "\n",
        "    return vec_env\n",
        "\n",
        "def record_with_proper_atari_tracking(model_path, output_path=\"fixed_atari_recording.mp4\",\n",
        "                                    env_id='SpaceInvadersNoFrameskip-v4', n_timesteps=5000):\n",
        "    \"\"\"\n",
        "    Record with proper Atari reward tracking but compatible observations\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== HYBRID APPROACH: RL Zoo3 Rewards + Compatible Observations ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "\n",
        "        # Create model-compatible environment (preprocessed)\n",
        "        model_env = create_model_compatible_env(env_id)\n",
        "        print(f\"Model environment obs space: {model_env.observation_space}\")\n",
        "\n",
        "        # Create separate raw environment for getting true Atari scores\n",
        "        import gymnasium as gym\n",
        "        raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "        print(f\"Raw environment obs space: {raw_env.observation_space}\")\n",
        "\n",
        "        # Load model with compatible environment\n",
        "        print(f\"Loading model: {model_path}\")\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "        print(\"‚úì Model loaded successfully\")\n",
        "\n",
        "        # Video recording setup\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = 60\n",
        "        all_frames = []\n",
        "\n",
        "        # Reset both environments\n",
        "        model_obs = model_env.reset()\n",
        "        raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "        print(f\"Model obs shape: {model_obs.shape}\")\n",
        "        print(f\"Raw obs shape: {raw_obs.shape}\")\n",
        "\n",
        "        # Tracking variables (like RL Zoo3)\n",
        "        episode_reward = 0.0  # Wrapper reward sum\n",
        "        step_count = 0\n",
        "        atari_scores = []\n",
        "        atari_lengths = []\n",
        "\n",
        "        print(f\"\\nStarting synchronized recording for {n_timesteps} timesteps...\")\n",
        "\n",
        "        for timestep in range(n_timesteps):\n",
        "            # Get action from model using preprocessed observations\n",
        "            action, _ = model.predict(model_obs, deterministic=True)\n",
        "\n",
        "            # Step both environments with same action\n",
        "            model_obs, model_reward, model_done, model_info = model_env.step(action)\n",
        "\n",
        "            # Extract scalar action for raw environment\n",
        "            raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "            raw_obs, raw_reward, raw_term, raw_trunc, raw_info = raw_env.step(raw_action)\n",
        "\n",
        "            # Track wrapper reward (like RL Zoo3 does internally)\n",
        "            episode_reward += model_reward[0]\n",
        "            step_count += 1\n",
        "\n",
        "            # Capture frame from raw environment (high resolution)\n",
        "            try:\n",
        "                frame = raw_env.render()\n",
        "                if frame is not None:\n",
        "                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    all_frames.append(frame_bgr)\n",
        "            except Exception as e:\n",
        "                if timestep < 5:\n",
        "                    print(f\"Frame capture warning: {e}\")\n",
        "\n",
        "            # CRITICAL: Check for Atari episode completion in raw_info\n",
        "            if raw_info and 'episode' in raw_info:\n",
        "                episode_info = raw_info['episode']\n",
        "                atari_score = episode_info['r']\n",
        "                atari_length = episode_info['l']\n",
        "\n",
        "                print(f\"üéÆ Atari Episode Score: {atari_score:.2f}\")\n",
        "                print(f\"üéÆ Atari Episode Length: {atari_length}\")\n",
        "                print(f\"   Wrapper reward sum: {episode_reward:.2f}\")\n",
        "                print(f\"   Steps in this recording: {step_count}\")\n",
        "\n",
        "                atari_scores.append(atari_score)\n",
        "                atari_lengths.append(atari_length)\n",
        "\n",
        "                # Reset tracking\n",
        "                episode_reward = 0.0\n",
        "                step_count = 0\n",
        "\n",
        "            # Progress indicator\n",
        "            if timestep % 1000 == 0 and timestep > 0:\n",
        "                print(f\"Progress: {timestep}/{n_timesteps} timesteps, \"\n",
        "                      f\"Episodes: {len(atari_scores)}, \"\n",
        "                      f\"Frames: {len(all_frames)}\")\n",
        "\n",
        "            # Check if we should stop (got enough episodes or model environment is done)\n",
        "            if model_done[0]:\n",
        "                print(f\"Model environment signaled done at timestep {timestep}\")\n",
        "                # Reset model environment but continue\n",
        "                model_obs = model_env.reset()\n",
        "\n",
        "        # Final results\n",
        "        print(f\"\\n=== RECORDING RESULTS ===\")\n",
        "        if atari_scores:\n",
        "            print(f\"‚úÖ Atari Episodes: {len(atari_scores)}\")\n",
        "            print(f\"üìä Scores: {atari_scores}\")\n",
        "            print(f\"üìè Lengths: {atari_lengths}\")\n",
        "            print(f\"üéØ Mean Score: {np.mean(atari_scores):.2f} ¬± {np.std(atari_scores):.2f}\")\n",
        "            print(f\"üìê Mean Length: {np.mean(atari_lengths):.2f} ¬± {np.std(atari_lengths):.2f}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No complete Atari episodes detected!\")\n",
        "            print(f\"   Total timesteps: {timestep + 1}\")\n",
        "            print(f\"   Current wrapper reward: {episode_reward:.2f}\")\n",
        "            print(\"   Try increasing n_timesteps or check episode detection\")\n",
        "\n",
        "        # Save video\n",
        "        if all_frames:\n",
        "            print(f\"\\nüé• Saving video with {len(all_frames)} frames...\")\n",
        "            h, w = all_frames[0].shape[:2]\n",
        "            out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "            for i, frame in enumerate(all_frames):\n",
        "                out.write(frame)\n",
        "                if i % 1500 == 0:  # Progress every 50 seconds at 30fps\n",
        "                    print(f\"   Writing: {i}/{len(all_frames)} frames ({100*i/len(all_frames):.1f}%)\")\n",
        "\n",
        "            out.release()\n",
        "\n",
        "            # File statistics\n",
        "            file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "            duration = len(all_frames) / fps\n",
        "            print(f\"‚úÖ Video saved: {output_path}\")\n",
        "            print(f\"üìÅ Size: {file_size:.1f} MB\")\n",
        "            print(f\"‚è±Ô∏è  Duration: {duration:.1f} seconds\")\n",
        "\n",
        "            if atari_scores:\n",
        "                episodes_per_minute = len(atari_scores) / (duration / 60)\n",
        "                print(f\"üéÆ Episodes/minute: {episodes_per_minute:.1f}\")\n",
        "        else:\n",
        "            print(\"‚ùå No frames captured for video!\")\n",
        "\n",
        "        # Cleanup\n",
        "        model_env.close()\n",
        "        raw_env.close()\n",
        "\n",
        "        return len(atari_scores) > 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in hybrid recording: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def debug_episode_detection(model_path, env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Debug why episodes might not be detected properly\n",
        "    \"\"\"\n",
        "    print(\"=== DEBUGGING EPISODE DETECTION ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "        import gymnasium as gym\n",
        "\n",
        "        # Create environments\n",
        "        model_env = create_model_compatible_env(env_id)\n",
        "        raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        # Load model\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "\n",
        "        # Reset\n",
        "        model_obs = model_env.reset()\n",
        "        raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "        print(f\"Initial raw_info: {raw_info}\")\n",
        "\n",
        "        episode_reward = 0\n",
        "        wrapper_reward = 0\n",
        "\n",
        "        for step in range(2000):  # Limit for debugging\n",
        "            # Get action and step both environments\n",
        "            action, _ = model.predict(model_obs, deterministic=True)\n",
        "            model_obs, m_reward, m_done, m_info = model_env.step(action)\n",
        "\n",
        "            raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "            raw_obs, r_reward, r_term, r_trunc, r_info = raw_env.step(raw_action)\n",
        "\n",
        "            episode_reward += r_reward\n",
        "            wrapper_reward += m_reward[0]\n",
        "\n",
        "            # Log interesting events\n",
        "            if (step < 10 or\n",
        "                step % 100 == 0 or\n",
        "                abs(r_reward) > 0 or\n",
        "                r_info or\n",
        "                m_done[0] or\n",
        "                r_term or r_trunc):\n",
        "\n",
        "                print(f\"Step {step:4d}: raw_r={r_reward:5.1f}, wrap_r={m_reward[0]:5.1f}, \"\n",
        "                      f\"done={m_done[0]}, term={r_term}, trunc={r_trunc}\")\n",
        "\n",
        "                if r_info:\n",
        "                    print(f\"          raw_info: {r_info}\")\n",
        "                if m_info:\n",
        "                    print(f\"          model_info: {m_info}\")\n",
        "\n",
        "                # Check for episode completion\n",
        "                if r_info and 'episode' in r_info:\n",
        "                    episode_info = r_info['episode']\n",
        "                    print(f\"üéØ EPISODE COMPLETE!\")\n",
        "                    print(f\"   True Atari Score: {episode_info['r']:.2f}\")\n",
        "                    print(f\"   Episode Length: {episode_info['l']}\")\n",
        "                    print(f\"   Raw reward sum: {episode_reward:.2f}\")\n",
        "                    print(f\"   Wrapper reward sum: {wrapper_reward:.2f}\")\n",
        "                    break\n",
        "\n",
        "            if m_done[0]:\n",
        "                print(f\"Model environment done at step {step}\")\n",
        "                break\n",
        "\n",
        "        model_env.close()\n",
        "        raw_env.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Debug failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(\"First, let's debug episode detection...\")\n",
        "        debug_episode_detection(MODEL_PATH)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Now recording with hybrid approach...\")\n",
        "\n",
        "        success = record_with_proper_atari_tracking(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"hybrid_atari_recording.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            n_timesteps=10000  # Increased to capture more episodes\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            print(\"\\nüéâ SUCCESS! Video should now show proper Atari episodes!\")\n",
        "            print(\"This matches RL Zoo3's evaluation but with working observations.\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Recording had issues. Check the debug output above.\")\n",
        "    else:\n",
        "        print(f\"Model not found: {MODEL_PATH}\")\n",
        "        print(\"Please update MODEL_PATH with the correct path.\")"
      ],
      "metadata": {
        "id": "Ww0CsDi9_8yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final rendering block\n",
        "\n",
        "Finally we see that we're tagging the wrong dict item in the raw rewards with 'epsiode'. We need to key on 'lives' instead and this will efficiently truncate our rendering to end after the loss of the last life - consistent with when one would expect a classic Space Invaders game to end"
      ],
      "metadata": {
        "id": "84bTjQCVkFOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make use of the 'lives' info in the r_info info dict, not the nonexistant 'episode' dict\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "def record_with_life_episode_detection(model_path, output_path=\"life_based_recording.mp4\",\n",
        "                                     env_id='SpaceInvadersNoFrameskip-v4', max_episodes=3):\n",
        "    \"\"\"\n",
        "    Record Atari episodes using life-based episode detection and manual score tracking\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== LIFE-BASED EPISODE DETECTION ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "\n",
        "        # Create model-compatible environment (preprocessed)\n",
        "        def make_env():\n",
        "            env = gym.make(env_id, render_mode='rgb_array')\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        model_env = DummyVecEnv([make_env])\n",
        "        model_env = VecFrameStack(model_env, n_stack=4)\n",
        "\n",
        "        # Create separate raw environment for rendering and score tracking\n",
        "        raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        print(f\"Model env obs space: {model_env.observation_space}\")\n",
        "        print(f\"Raw env obs space: {raw_env.observation_space}\")\n",
        "\n",
        "        # Load model\n",
        "        print(f\"Loading model: {model_path}\")\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "        print(\"‚úì Model loaded successfully\")\n",
        "\n",
        "        # Video recording setup\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = 60\n",
        "        all_frames = []\n",
        "\n",
        "        # Episode tracking variables\n",
        "        episodes_completed = 0\n",
        "        episode_scores = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        while episodes_completed < max_episodes:\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"STARTING EPISODE {episodes_completed + 1}/{max_episodes}\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            # Reset both environments\n",
        "            model_obs = model_env.reset()\n",
        "            raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "            # Episode state tracking\n",
        "            current_lives = raw_info.get('lives', 3)\n",
        "            initial_lives = current_lives\n",
        "            episode_score = 0\n",
        "            episode_steps = 0\n",
        "            life_lost_recently = False\n",
        "\n",
        "            print(f\"Starting with {current_lives} lives\")\n",
        "\n",
        "            # Play until all lives are lost\n",
        "            while current_lives > 0:\n",
        "                # Get action from model\n",
        "                action, _ = model.predict(model_obs, deterministic=True)\n",
        "\n",
        "                # Step both environments\n",
        "                model_obs, model_reward, model_done, model_info = model_env.step(action)\n",
        "\n",
        "                raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "                raw_obs, raw_reward, raw_term, raw_trunc, raw_info = raw_env.step(raw_action)\n",
        "\n",
        "                # Track score and steps\n",
        "                episode_score += raw_reward\n",
        "                episode_steps += 1\n",
        "\n",
        "                # Capture frame\n",
        "                try:\n",
        "                    frame = raw_env.render()\n",
        "                    if frame is not None:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                        all_frames.append(frame_bgr)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                # Check for life loss\n",
        "                new_lives = raw_info.get('lives', current_lives)\n",
        "                if new_lives < current_lives:\n",
        "                    print(f\"  Life lost! Lives: {current_lives} ‚Üí {new_lives}, Score: {episode_score}, Steps: {episode_steps}\")\n",
        "                    current_lives = new_lives\n",
        "                    life_lost_recently = True\n",
        "\n",
        "                # Check if model environment reset (happens on life loss in wrapped env)\n",
        "                if model_done[0]:\n",
        "                    print(f\"  Model environment reset at step {episode_steps}\")\n",
        "                    model_obs = model_env.reset()\n",
        "\n",
        "                # Progress indicator\n",
        "                if episode_steps % 500 == 0:\n",
        "                    print(f\"  Progress: {episode_steps} steps, Score: {episode_score:.0f}, Lives: {current_lives}\")\n",
        "\n",
        "                # Safety break for very long episodes\n",
        "                if episode_steps > 20000:\n",
        "                    print(f\"  Episode exceeded 20000 steps, ending...\")\n",
        "                    break\n",
        "\n",
        "            # Episode completed (all lives lost)\n",
        "            episode_scores.append(episode_score)\n",
        "            episode_lengths.append(episode_steps)\n",
        "            episodes_completed += 1\n",
        "\n",
        "            print(f\"\\nüéÆ EPISODE {episodes_completed} COMPLETED!\")\n",
        "            print(f\"   Final Score: {episode_score:.0f}\")\n",
        "            print(f\"   Episode Length: {episode_steps} steps\")\n",
        "            print(f\"   Lives Used: {initial_lives}\")\n",
        "\n",
        "            # Add a few seconds of the final screen\n",
        "            print(\"   Recording final screen for 3 seconds...\")\n",
        "            for _ in range(90):  # 3 seconds at 30fps\n",
        "                try:\n",
        "                    frame = raw_env.render()\n",
        "                    if frame is not None:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                        all_frames.append(frame_bgr)\n",
        "                except:\n",
        "                    break\n",
        "\n",
        "        # Final results\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"RECORDING SUMMARY\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"‚úÖ Episodes Completed: {episodes_completed}\")\n",
        "        print(f\"üìä Episode Scores: {episode_scores}\")\n",
        "        print(f\"üìè Episode Lengths: {episode_lengths}\")\n",
        "        print(f\"üéØ Mean Score: {np.mean(episode_scores):.1f} ¬± {np.std(episode_scores):.1f}\")\n",
        "        print(f\"üìê Mean Length: {np.mean(episode_lengths):.1f} ¬± {np.std(episode_lengths):.1f}\")\n",
        "        print(f\"üé¨ Total Frames: {len(all_frames)}\")\n",
        "\n",
        "        # Compare with RL Zoo3 results\n",
        "        print(f\"\\nüìà COMPARISON WITH RL ZOO3:\")\n",
        "        print(f\"   RL Zoo3 reported: 275-600 points, 2000-5000 steps\")\n",
        "        print(f\"   This recording: {min(episode_scores):.0f}-{max(episode_scores):.0f} points, {min(episode_lengths)}-{max(episode_lengths)} steps\")\n",
        "\n",
        "        # Save video\n",
        "        if all_frames:\n",
        "            print(f\"\\nüé• SAVING VIDEO...\")\n",
        "            h, w = all_frames[0].shape[:2]\n",
        "            out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "            for i, frame in enumerate(all_frames):\n",
        "                out.write(frame)\n",
        "                if i % 1500 == 0:\n",
        "                    print(f\"   Writing: {i}/{len(all_frames)} frames ({100*i/len(all_frames):.1f}%)\")\n",
        "\n",
        "            out.release()\n",
        "\n",
        "            # File statistics\n",
        "            file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "            duration = len(all_frames) / fps\n",
        "            print(f\"‚úÖ Video saved: {output_path}\")\n",
        "            print(f\"üìÅ Size: {file_size:.1f} MB\")\n",
        "            print(f\"‚è±Ô∏è  Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
        "            print(f\"üéÆ Episodes per minute: {episodes_completed / (duration/60):.1f}\")\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå No frames captured!\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in life-based recording: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        try:\n",
        "            model_env.close()\n",
        "            raw_env.close()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "def quick_life_test(model_path, env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Quick test to understand the life mechanics\n",
        "    \"\"\"\n",
        "    print(\"=== QUICK LIFE MECHANICS TEST ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "\n",
        "        # Create environments\n",
        "        def make_env():\n",
        "            env = gym.make(env_id, render_mode='rgb_array')\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        model_env = DummyVecEnv([make_env])\n",
        "        model_env = VecFrameStack(model_env, n_stack=4)\n",
        "        raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        # Load model\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "\n",
        "        # Reset and test\n",
        "        model_obs = model_env.reset()\n",
        "        raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "        print(f\"Initial raw_info: {raw_info}\")\n",
        "\n",
        "        current_lives = raw_info.get('lives', 3)\n",
        "        total_score = 0\n",
        "        step_count = 0\n",
        "\n",
        "        print(f\"Starting with {current_lives} lives\")\n",
        "\n",
        "        # Run until we see some life changes\n",
        "        life_changes = 0\n",
        "        while life_changes < 2 and step_count < 5000:\n",
        "            action, _ = model.predict(model_obs, deterministic=True)\n",
        "\n",
        "            model_obs, m_reward, m_done, m_info = model_env.step(action)\n",
        "            raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "            raw_obs, r_reward, r_term, r_trunc, r_info = raw_env.step(raw_action)\n",
        "\n",
        "            total_score += r_reward\n",
        "            step_count += 1\n",
        "\n",
        "            # Check for life changes\n",
        "            new_lives = r_info.get('lives', current_lives)\n",
        "            if new_lives != current_lives:\n",
        "                life_changes += 1\n",
        "                print(f\"\\nüîÑ LIFE CHANGE #{life_changes} at step {step_count}:\")\n",
        "                print(f\"   Lives: {current_lives} ‚Üí {new_lives}\")\n",
        "                print(f\"   Score so far: {total_score:.0f}\")\n",
        "                print(f\"   Model done: {m_done[0]}\")\n",
        "                print(f\"   Raw info: {r_info}\")\n",
        "                current_lives = new_lives\n",
        "\n",
        "                if m_done[0]:\n",
        "                    print(f\"   Model environment reset\")\n",
        "                    model_obs = model_env.reset()\n",
        "\n",
        "            # Log rewards\n",
        "            if abs(r_reward) > 0:\n",
        "                print(f\"Step {step_count}: +{r_reward:.0f} points (total: {total_score:.0f})\")\n",
        "\n",
        "        print(f\"\\nTest completed:\")\n",
        "        print(f\"  Total score: {total_score:.0f}\")\n",
        "        print(f\"  Steps: {step_count}\")\n",
        "        print(f\"  Final lives: {current_lives}\")\n",
        "        print(f\"  Life changes observed: {life_changes}\")\n",
        "\n",
        "        model_env.close()\n",
        "        raw_env.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Life test failed: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(\"First, running quick life mechanics test...\")\n",
        "        quick_life_test(MODEL_PATH)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Now recording with life-based episode detection...\")\n",
        "\n",
        "        success = record_with_life_episode_detection(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"life_based_spaceinvaders.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            max_episodes=2  # Record 2 complete episodes\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            print(\"\\nüéâ SUCCESS! Video shows complete Atari episodes with proper scoring!\")\n",
        "            print(\"The scores should now match RL Zoo3's evaluation results.\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Recording failed. Check the error messages above.\")\n",
        "    else:\n",
        "        print(f\"Model not found: {MODEL_PATH}\")\n",
        "        print(\"Please update MODEL_PATH with the correct path.\")"
      ],
      "metadata": {
        "id": "4rVoTo6YAjVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further investigation\n",
        "\n",
        "While we have no effectively rendered our episode to the expected point of completion, the reward and episode length discrepancy remains. We evaluate the separate reward values side by side"
      ],
      "metadata": {
        "id": "wXKfxMJemqFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "def record_with_life_episode_detection(model_path, output_path=\"life_based_recording.mp4\",\n",
        "                                     env_id='SpaceInvadersNoFrameskip-v4', max_episodes=3):\n",
        "    \"\"\"\n",
        "    Record Atari episodes using life-based episode detection and manual score tracking\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== LIFE-BASED EPISODE DETECTION ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "\n",
        "        # Create model-compatible environment (preprocessed)\n",
        "        def make_env():\n",
        "            env = gym.make(env_id, render_mode='rgb_array')\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        model_env = DummyVecEnv([make_env])\n",
        "        model_env = VecFrameStack(model_env, n_stack=4)\n",
        "\n",
        "        # Create separate raw environment for rendering and score tracking\n",
        "        raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        print(f\"Model env obs space: {model_env.observation_space}\")\n",
        "        print(f\"Raw env obs space: {raw_env.observation_space}\")\n",
        "\n",
        "        # Load model\n",
        "        print(f\"Loading model: {model_path}\")\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "        print(\"‚úì Model loaded successfully\")\n",
        "\n",
        "        # Video recording setup\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = 60\n",
        "        all_frames = []\n",
        "\n",
        "        # Episode tracking variables\n",
        "        episodes_completed = 0\n",
        "        episode_scores = []  # True Atari scores\n",
        "        model_rewards = []   # Wrapper rewards\n",
        "        episode_lengths = []\n",
        "\n",
        "        while episodes_completed < max_episodes:\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"STARTING EPISODE {episodes_completed + 1}/{max_episodes}\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            # Reset both environments\n",
        "            model_obs = model_env.reset()\n",
        "            raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "            # Episode state tracking\n",
        "            current_lives = raw_info.get('lives', 3)\n",
        "            initial_lives = current_lives\n",
        "            episode_score = 0  # True Atari score from raw environment\n",
        "            model_reward_sum = 0  # Wrapper reward from model environment\n",
        "            episode_steps = 0\n",
        "            life_lost_recently = False\n",
        "\n",
        "            print(f\"Starting with {current_lives} lives\")\n",
        "\n",
        "            # Play until all lives are lost\n",
        "            while current_lives > 0:\n",
        "                # Get action from model\n",
        "                action, _ = model.predict(model_obs, deterministic=True)\n",
        "\n",
        "                # Step both environments\n",
        "                model_obs, model_reward, model_done, model_info = model_env.step(action)\n",
        "\n",
        "                raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "                raw_obs, raw_reward, raw_term, raw_trunc, raw_info = raw_env.step(raw_action)\n",
        "\n",
        "                # Track both scores\n",
        "                episode_score += raw_reward  # True Atari score\n",
        "                model_reward_sum += model_reward[0]  # Wrapper reward\n",
        "                episode_steps += 1\n",
        "\n",
        "                # Capture frame\n",
        "                try:\n",
        "                    frame = raw_env.render()\n",
        "                    if frame is not None:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                        all_frames.append(frame_bgr)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                # Check for life loss\n",
        "                new_lives = raw_info.get('lives', current_lives)\n",
        "                if new_lives < current_lives:\n",
        "                    print(f\"  Life lost! Lives: {current_lives} ‚Üí {new_lives}\")\n",
        "                    print(f\"    True Atari Score: {episode_score}\")\n",
        "                    print(f\"    Model Wrapper Reward: {model_reward_sum:.1f}\")\n",
        "                    print(f\"    Steps: {episode_steps}\")\n",
        "                    current_lives = new_lives\n",
        "                    life_lost_recently = True\n",
        "\n",
        "                # Check if model environment reset (happens on life loss in wrapped env)\n",
        "                if model_done[0]:\n",
        "                    print(f\"  Model environment reset at step {episode_steps}\")\n",
        "                    model_obs = model_env.reset()\n",
        "\n",
        "                # Progress indicator\n",
        "                if episode_steps % 500 == 0:\n",
        "                    print(f\"  Progress: {episode_steps} steps\")\n",
        "                    print(f\"    True Score: {episode_score:.0f}, Wrapper Reward: {model_reward_sum:.1f}\")\n",
        "                    print(f\"    Lives: {current_lives}\")\n",
        "\n",
        "                # Safety break for very long episodes\n",
        "                if episode_steps > 20000:\n",
        "                    print(f\"  Episode exceeded 20000 steps, ending...\")\n",
        "                    break\n",
        "\n",
        "            # Episode completed (all lives lost)\n",
        "            episode_scores.append(episode_score)\n",
        "            model_rewards.append(model_reward_sum)\n",
        "            episode_lengths.append(episode_steps)\n",
        "            episodes_completed += 1\n",
        "\n",
        "            print(f\"\\nüéÆ EPISODE {episodes_completed} COMPLETED!\")\n",
        "            print(f\"   True Atari Score: {episode_score:.0f}\")\n",
        "            print(f\"   Model Wrapper Reward: {model_reward_sum:.1f}\")\n",
        "            print(f\"   Episode Length: {episode_steps} steps\")\n",
        "            print(f\"   Lives Used: {initial_lives}\")\n",
        "            print(f\"   Score Ratio (True/Wrapper): {episode_score/model_reward_sum:.1f}x\" if model_reward_sum != 0 else \"   Score Ratio: N/A (no wrapper reward)\")\n",
        "\n",
        "            # Add a few seconds of the final screen\n",
        "            print(\"   Recording final screen for 3 seconds...\")\n",
        "            for _ in range(180):  # 3 seconds at 30fps\n",
        "                try:\n",
        "                    frame = raw_env.render()\n",
        "                    if frame is not None:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                        all_frames.append(frame_bgr)\n",
        "                except:\n",
        "                    break\n",
        "\n",
        "        # Final results\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"RECORDING SUMMARY\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"‚úÖ Episodes Completed: {episodes_completed}\")\n",
        "        print(f\"üìä True Atari Scores: {episode_scores}\")\n",
        "        print(f\"üîß Model Wrapper Rewards: {[f'{r:.1f}' for r in model_rewards]}\")\n",
        "        print(f\"üìè Episode Lengths: {episode_lengths}\")\n",
        "        print(f\"üéØ Mean True Score: {np.mean(episode_scores):.1f} ¬± {np.std(episode_scores):.1f}\")\n",
        "        print(f\"üîß Mean Wrapper Reward: {np.mean(model_rewards):.1f} ¬± {np.std(model_rewards):.1f}\")\n",
        "        print(f\"üìê Mean Length: {np.mean(episode_lengths):.1f} ¬± {np.std(episode_lengths):.1f}\")\n",
        "        print(f\"üé¨ Total Frames: {len(all_frames)}\")\n",
        "\n",
        "        # Reward comparison analysis\n",
        "        print(f\"\\nüîç REWARD COMPARISON ANALYSIS:\")\n",
        "        for i, (true_score, wrapper_reward) in enumerate(zip(episode_scores, model_rewards)):\n",
        "            ratio = true_score / wrapper_reward if wrapper_reward != 0 else float('inf')\n",
        "            print(f\"   Episode {i+1}: {true_score:.0f} true vs {wrapper_reward:.1f} wrapper (ratio: {ratio:.1f}x)\")\n",
        "\n",
        "        avg_ratio = np.mean([s/w for s, w in zip(episode_scores, model_rewards) if w != 0])\n",
        "        print(f\"   Average ratio: {avg_ratio:.1f}x (true score is {avg_ratio:.1f}x larger than wrapper reward)\")\n",
        "\n",
        "        print(f\"\\nüìà COMPARISON WITH RL ZOO3:\")\n",
        "        print(f\"   RL Zoo3 reported: 275-600 points, 2000-5000 steps\")\n",
        "        print(f\"   This recording (true): {min(episode_scores):.0f}-{max(episode_scores):.0f} points, {min(episode_lengths)}-{max(episode_lengths)} steps\")\n",
        "        print(f\"   This recording (wrapper): {min(model_rewards):.1f}-{max(model_rewards):.1f} rewards\")\n",
        "        print(f\"   ‚úÖ True scores match RL Zoo3!\" if min(episode_scores) >= 200 else \"   ‚ö†Ô∏è  True scores seem low compared to RL Zoo3\")\n",
        "\n",
        "        # Save video\n",
        "        if all_frames:\n",
        "            print(f\"\\nüé• SAVING VIDEO...\")\n",
        "            h, w = all_frames[0].shape[:2]\n",
        "            out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "            for i, frame in enumerate(all_frames):\n",
        "                out.write(frame)\n",
        "                if i % 1500 == 0:\n",
        "                    print(f\"   Writing: {i}/{len(all_frames)} frames ({100*i/len(all_frames):.1f}%)\")\n",
        "\n",
        "            out.release()\n",
        "\n",
        "            # File statistics\n",
        "            file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "            duration = len(all_frames) / fps\n",
        "            print(f\"‚úÖ Video saved: {output_path}\")\n",
        "            print(f\"üìÅ Size: {file_size:.1f} MB\")\n",
        "            print(f\"‚è±Ô∏è  Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
        "            print(f\"üéÆ Episodes per minute: {episodes_completed / (duration/60):.1f}\")\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå No frames captured!\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in life-based recording: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        try:\n",
        "            model_env.close()\n",
        "            raw_env.close()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "def quick_life_test(model_path, env_id='SpaceInvadersNoFrameskip-v4'):\n",
        "    \"\"\"\n",
        "    Quick test to understand the life mechanics\n",
        "    \"\"\"\n",
        "    print(\"=== QUICK LIFE MECHANICS TEST ===\")\n",
        "\n",
        "    try:\n",
        "        from rl_zoo3 import ALGOS\n",
        "\n",
        "        # Create environments\n",
        "        def make_env():\n",
        "            env = gym.make(env_id, render_mode='rgb_array')\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        model_env = DummyVecEnv([make_env])\n",
        "        model_env = VecFrameStack(model_env, n_stack=4)\n",
        "        raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "        # Load model\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "\n",
        "        # Reset and test\n",
        "        model_obs = model_env.reset()\n",
        "        raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "        print(f\"Initial raw_info: {raw_info}\")\n",
        "\n",
        "        current_lives = raw_info.get('lives', 3)\n",
        "        total_score = 0  # True Atari score\n",
        "        model_total = 0  # Wrapper reward\n",
        "        step_count = 0\n",
        "\n",
        "        print(f\"Starting with {current_lives} lives\")\n",
        "\n",
        "        # Run until we see some life changes\n",
        "        life_changes = 0\n",
        "        while life_changes < 2 and step_count < 5000:\n",
        "            action, _ = model.predict(model_obs, deterministic=True)\n",
        "\n",
        "            model_obs, m_reward, m_done, m_info = model_env.step(action)\n",
        "            raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "            raw_obs, r_reward, r_term, r_trunc, r_info = raw_env.step(raw_action)\n",
        "\n",
        "            total_score += r_reward  # True score\n",
        "            model_total += m_reward[0]  # Wrapper reward\n",
        "            step_count += 1\n",
        "\n",
        "            # Check for life changes\n",
        "            new_lives = r_info.get('lives', current_lives)\n",
        "            if new_lives != current_lives:\n",
        "                life_changes += 1\n",
        "                print(f\"\\nüîÑ LIFE CHANGE #{life_changes} at step {step_count}:\")\n",
        "                print(f\"   Lives: {current_lives} ‚Üí {new_lives}\")\n",
        "                print(f\"   True score so far: {total_score:.0f}\")\n",
        "                print(f\"   Wrapper reward so far: {model_total:.1f}\")\n",
        "                print(f\"   Model done: {m_done[0]}\")\n",
        "                print(f\"   Raw info: {r_info}\")\n",
        "                current_lives = new_lives\n",
        "\n",
        "                if m_done[0]:\n",
        "                    print(f\"   Model environment reset\")\n",
        "                    model_obs = model_env.reset()\n",
        "\n",
        "            # Log rewards\n",
        "            if abs(r_reward) > 0 or abs(m_reward[0]) > 0:\n",
        "                print(f\"Step {step_count}: True +{r_reward:.0f}, Wrapper +{m_reward[0]:.1f} (totals: {total_score:.0f} / {model_total:.1f})\")\n",
        "\n",
        "        print(f\"\\nTest completed:\")\n",
        "        print(f\"  True Atari score: {total_score:.0f}\")\n",
        "        print(f\"  Model wrapper reward: {model_total:.1f}\")\n",
        "        print(f\"  Steps: {step_count}\")\n",
        "        print(f\"  Final lives: {current_lives}\")\n",
        "        print(f\"  Life changes observed: {life_changes}\")\n",
        "        print(f\"  Score ratio (true/wrapper): {total_score/model_total:.1f}x\" if model_total != 0 else \"  Score ratio: N/A\")\n",
        "\n",
        "        model_env.close()\n",
        "        raw_env.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Life test failed: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(\"First, running quick life mechanics test...\")\n",
        "        quick_life_test(MODEL_PATH)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Now recording with life-based episode detection...\")\n",
        "\n",
        "        success = record_with_life_episode_detection(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"life_based_spaceinvaders.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            max_episodes=2  # Record 2 complete episodes\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            print(\"\\nüéâ SUCCESS! Video shows complete Atari episodes with proper scoring!\")\n",
        "            print(\"The scores should now match RL Zoo3's evaluation results.\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Recording failed. Check the error messages above.\")\n",
        "    else:\n",
        "        print(f\"Model not found: {MODEL_PATH}\")\n",
        "        print(\"Please update MODEL_PATH with the correct path.\")"
      ],
      "metadata": {
        "id": "0H-zgQR7m5D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's run a number of evaluations and save the best result only"
      ],
      "metadata": {
        "id": "5nBT1CmV_Bow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def setup_environments():\n",
        "    \"\"\"Register ALE environments\"\"\"\n",
        "    import ale_py\n",
        "    import gymnasium as gym\n",
        "    gym.register_envs(ale_py)\n",
        "\n",
        "def evaluate_and_record_best_episode(model_path, output_path=\"best_episode.mp4\",\n",
        "                                   env_id='SpaceInvadersNoFrameskip-v4',\n",
        "                                   num_evaluation_episodes=5):\n",
        "    \"\"\"\n",
        "    Evaluate agent over multiple episodes, keeping frames for the best episode during evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=== EVALUATE AND RECORD BEST EPISODE ===\")\n",
        "\n",
        "    # Setup environments\n",
        "    setup_environments()\n",
        "\n",
        "    try:\n",
        "        import gymnasium as gym\n",
        "        from rl_zoo3 import ALGOS\n",
        "        from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "        from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "        # Create model-compatible environment (preprocessed)\n",
        "        def make_env():\n",
        "            env = gym.make(env_id, render_mode='rgb_array')\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        model_env = DummyVecEnv([make_env])\n",
        "        model_env = VecFrameStack(model_env, n_stack=4)\n",
        "\n",
        "        # Load model\n",
        "        print(f\"Loading model: {model_path}\")\n",
        "        model = ALGOS['dqn'].load(model_path, env=model_env, force_reset=True)\n",
        "        print(\"‚úì Model loaded successfully\")\n",
        "\n",
        "        print(f\"\\nüé¨ EVALUATING AND RECORDING {num_evaluation_episodes} EPISODES...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Memoized episode data\n",
        "        max_episode_score = -1\n",
        "        max_episode_frames = []\n",
        "        max_episode_data = {}\n",
        "\n",
        "        evaluation_results = []\n",
        "\n",
        "        for episode_num in range(num_evaluation_episodes):\n",
        "            print(f\"\\nEpisode {episode_num + 1}/{num_evaluation_episodes}...\")\n",
        "\n",
        "            # Create fresh raw environment for this evaluation\n",
        "            raw_env = gym.make(env_id, render_mode='rgb_array')\n",
        "\n",
        "            # Reset environments\n",
        "            model_obs = model_env.reset()\n",
        "            raw_obs, raw_info = raw_env.reset()\n",
        "\n",
        "            # Episode tracking\n",
        "            current_lives = raw_info.get('lives', 3)\n",
        "            episode_score = 0\n",
        "            model_reward_sum = 0\n",
        "            episode_steps = 0\n",
        "\n",
        "            # Current episode frames\n",
        "            current_episode_frames = []\n",
        "\n",
        "            # Play episode and capture frames\n",
        "            while current_lives > 0:\n",
        "                # Get action and step\n",
        "                action, _ = model.predict(model_obs, deterministic=True)\n",
        "                model_obs, model_reward, model_done, model_info = model_env.step(action)\n",
        "\n",
        "                raw_action = action[0] if hasattr(action, '__len__') else action\n",
        "                raw_obs, raw_reward, raw_term, raw_trunc, raw_info = raw_env.step(raw_action)\n",
        "\n",
        "                # Track scores\n",
        "                episode_score += raw_reward\n",
        "                model_reward_sum += model_reward[0]\n",
        "                episode_steps += 1\n",
        "\n",
        "                # Capture frame for this episode\n",
        "                try:\n",
        "                    frame = raw_env.render()\n",
        "                    if frame is not None:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                        current_episode_frames.append(frame_bgr)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                # Check for life loss\n",
        "                new_lives = raw_info.get('lives', current_lives)\n",
        "                if new_lives < current_lives:\n",
        "                    current_lives = new_lives\n",
        "\n",
        "                # Reset model env on done\n",
        "                if model_done[0]:\n",
        "                    model_obs = model_env.reset()\n",
        "\n",
        "                # Safety break\n",
        "                if episode_steps > 20000:\n",
        "                    break\n",
        "\n",
        "            # Add final screen frames (3 seconds)\n",
        "            for _ in range(180):  # 3 seconds at 60fps\n",
        "                try:\n",
        "                    frame = raw_env.render()\n",
        "                    if frame is not None:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                        current_episode_frames.append(frame_bgr)\n",
        "                except:\n",
        "                    break\n",
        "\n",
        "            # Store episode results\n",
        "            episode_result = {\n",
        "                'episode_num': episode_num + 1,\n",
        "                'true_score': episode_score,\n",
        "                'wrapper_reward': model_reward_sum,\n",
        "                'steps': episode_steps,\n",
        "                'frames_captured': len(current_episode_frames)\n",
        "            }\n",
        "            evaluation_results.append(episode_result)\n",
        "\n",
        "            # Check if this is the new best episode\n",
        "            if episode_score > max_episode_score:\n",
        "                print(f\"   üèÜ NEW BEST! Score: {episode_score:.0f} (previous best: {max_episode_score:.0f})\")\n",
        "\n",
        "                # Update max episode data\n",
        "                max_episode_score = episode_score\n",
        "                max_episode_frames = current_episode_frames.copy()  # Keep these frames\n",
        "                max_episode_data = episode_result.copy()\n",
        "\n",
        "            else:\n",
        "                print(f\"   Score: {episode_score:.0f} (best remains: {max_episode_score:.0f})\")\n",
        "\n",
        "            # Clean up current episode frames if not the best (save memory)\n",
        "            if episode_score != max_episode_score:\n",
        "                current_episode_frames.clear()\n",
        "\n",
        "            print(f\"   Steps: {episode_steps}, Wrapper: {model_reward_sum:.1f}, Frames: {len(current_episode_frames) if episode_score == max_episode_score else 'discarded'}\")\n",
        "\n",
        "            raw_env.close()\n",
        "\n",
        "        print(f\"\\nüìä FINAL EVALUATION RESULTS:\")\n",
        "        print(\"=\" * 60)\n",
        "        for result in evaluation_results:\n",
        "            marker = \" üèÜ BEST\" if result['true_score'] == max_episode_score else \"\"\n",
        "            print(f\"Episode {result['episode_num']}: \"\n",
        "                  f\"Score={result['true_score']:.0f}, \"\n",
        "                  f\"Wrapper={result['wrapper_reward']:.1f}, \"\n",
        "                  f\"Steps={result['steps']}, \"\n",
        "                  f\"Frames={result['frames_captured']}{marker}\")\n",
        "\n",
        "        print(f\"\\nüéØ Best Episode: #{max_episode_data['episode_num']} with score {max_episode_score:.0f}\")\n",
        "        print(f\"üìπ Frames captured: {len(max_episode_frames)}\")\n",
        "\n",
        "        # Save video of best episode\n",
        "        if max_episode_frames:\n",
        "            print(f\"\\nüíæ SAVING BEST EPISODE VIDEO...\")\n",
        "\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            fps = 60\n",
        "            h, w = max_episode_frames[0].shape[:2]\n",
        "            out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "            for i, frame in enumerate(max_episode_frames):\n",
        "                out.write(frame)\n",
        "                if i % 1000 == 0 and i > 0:\n",
        "                    print(f\"   Writing: {i}/{len(max_episode_frames)} frames ({100*i/len(max_episode_frames):.1f}%)\")\n",
        "\n",
        "            out.release()\n",
        "\n",
        "            # File info\n",
        "            file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "            duration = len(max_episode_frames) / fps\n",
        "\n",
        "            print(f\"\\nüéâ SUCCESS!\")\n",
        "            print(f\"üì∫ Video: {output_path}\")\n",
        "            print(f\"üìÅ Size: {file_size:.1f} MB\")\n",
        "            print(f\"‚è±Ô∏è  Duration: {duration:.1f} seconds\")\n",
        "            print(f\"üéÆ Episode Score: {max_episode_score:.0f}\")\n",
        "            print(f\"üîß Wrapper Reward: {max_episode_data['wrapper_reward']:.1f}\")\n",
        "            print(f\"üìä Steps: {max_episode_data['steps']}\")\n",
        "            print(f\"üèÜ Best of {num_evaluation_episodes} episodes\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå No frames captured for best episode!\")\n",
        "\n",
        "        # Cleanup\n",
        "        model_env.close()\n",
        "\n",
        "        return {\n",
        "            'evaluation_results': evaluation_results,\n",
        "            'best_episode': max_episode_data,\n",
        "            'best_score': max_episode_score,\n",
        "            'video_saved': len(max_episode_frames) > 0,\n",
        "            'total_frames': len(max_episode_frames)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in evaluation/recording: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_PATH = \"logs/dqn/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\"\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        results = evaluate_and_record_best_episode(\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=\"best_spaceinvaders_episode.mp4\",\n",
        "            env_id='SpaceInvadersNoFrameskip-v4',\n",
        "            num_evaluation_episodes=15  # Evaluate 5 episodes, record the best one\n",
        "        )\n",
        "\n",
        "        if results and results['video_saved']:\n",
        "            print(f\"\\n‚úÖ Successfully recorded the best episode!\")\n",
        "            print(f\"   Evaluated {len(results['evaluation_results'])} episodes\")\n",
        "            print(f\"   Best score: {results['best_episode']['true_score']:.0f}\")\n",
        "            print(f\"   Video shows episode with score: {results['best_score']:.0f}\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Failed to record the best episode.\")\n",
        "    else:\n",
        "        print(f\"Model not found: {MODEL_PATH}\")\n",
        "        print(\"Please update MODEL_PATH with the correct path.\")"
      ],
      "metadata": {
        "id": "wuCdgytb2J3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9A-G9gij_KTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
